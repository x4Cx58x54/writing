
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
        <meta name="author" content="x4Cx58x54">
      
      
      
        <link rel="prev" href="../statistics/">
      
      
        <link rel="next" href="../matrix-algebra/">
      
      <link rel="icon" href="../../none">
      <meta name="generator" content="mkdocs-1.4.2, mkdocs-material-9.1.4">
    
    
      
        <title>Elements of Information Theory - Randomish Writing</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.240905d7.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.a0c5b2b5.min.css">
      
      

    
    
    
      
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css">
    
      <link rel="stylesheet" href="../../_static/css/extra.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      
  


  
  


  
    <script>"undefined"!=typeof __md_analytics&&__md_analytics()</script>
  

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="white" data-md-color-accent="light-blue">
  
    
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#elements-of-information-theory" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Randomish Writing" class="md-header__button md-logo" aria-label="Randomish Writing" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 9h16v2H4V9m0 4h10v2H4v-2Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Randomish Writing
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Elements of Information Theory
            
          </span>
        </div>
      </div>
    </div>
    
    
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  


  <li class="md-tabs__item">
    <a href="../.." class="md-tabs__link">
      Welcome
    </a>
  </li>

      
        
  
  
    
  


  
  
  
    <li class="md-tabs__item">
      <a href="../essence-of-linear-algebra/" class="md-tabs__link md-tabs__link--active">
        Mathematics
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../c/c-primer-plus/" class="md-tabs__link">
        Languages
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../ai/deep-learning/" class="md-tabs__link">
        AI
      </a>
    </li>
  

      
        
  
  


  
  
  
    

  
  
  
    <li class="md-tabs__item">
      <a href="../../competitive-programming/prime/" class="md-tabs__link">
        Competitive Programming
      </a>
    </li>
  

  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../computer-organization/computer-organization/" class="md-tabs__link">
        CS
      </a>
    </li>
  

      
        
  
  


  
  
  
    

  
  
  
    <li class="md-tabs__item">
      <a href="../../guides/linux/linux-initial-setup/" class="md-tabs__link">
        Guides
      </a>
    </li>
  

  

      
        
  
  


  
  
  
    

  
  
  
    <li class="md-tabs__item">
      <a href="../../electromagnetics/electromagnetics/" class="md-tabs__link">
        Misc
      </a>
    </li>
  

  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../scribbles/" class="md-tabs__link">
        Scribbles
      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Randomish Writing" class="md-nav__button md-logo" aria-label="Randomish Writing" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 9h16v2H4V9m0 4h10v2H4v-2Z"/></svg>

    </a>
    Randomish Writing
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        Welcome
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
      
      
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
          Mathematics
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          Mathematics
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../essence-of-linear-algebra/" class="md-nav__link">
        Essence of Linear Algebra
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../linear-algebra/" class="md-nav__link">
        Linear Algebra
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_3" >
      
      
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_2_3" id="__nav_2_3_label" tabindex="0">
          Probability Theory
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_3">
          <span class="md-nav__icon md-icon"></span>
          Probability Theory
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../probability-theory/" class="md-nav__link">
        Probability Theory
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../combinatorics/" class="md-nav__link">
        Combinatorics Example Problems
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_3_3" >
      
      
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_2_3_3" id="__nav_2_3_3_label" tabindex="0">
          Problemsets
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_3_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_3_3">
          <span class="md-nav__icon md-icon"></span>
          Problemsets
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../events-and-probability/" class="md-nav__link">
        Events and Probability
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../random-variables/" class="md-nav__link">
        Random Variables
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../statistics/" class="md-nav__link">
        Statistics
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          Elements of Information Theory
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        Elements of Information Theory
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#ch-2-entropy-relative-entropy-mutual-information" class="md-nav__link">
    Ch. 2: Entropy, relative entropy, mutual information
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ch-3-asymptotic-equipartition-property" class="md-nav__link">
    Ch. 3: Asymptotic equipartition property
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ch-4-entropy-rates-of-a-stochastic-process" class="md-nav__link">
    Ch. 4: Entropy rates of a stochastic process
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ch-5-data-compression" class="md-nav__link">
    Ch. 5: Data compression
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ch-7-channel-capacity" class="md-nav__link">
    Ch. 7: Channel capacity
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ch-8-differential-entropy" class="md-nav__link">
    Ch. 8: Differential entropy
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ch-9-gaussian-channel" class="md-nav__link">
    Ch. 9: Gaussian channel
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ch-10-rate-distortion-theory" class="md-nav__link">
    Ch. 10: Rate distortion theory
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ch-11-information-theory-and-statistics" class="md-nav__link">
    Ch. 11: Information theory and statistics
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../matrix-algebra/" class="md-nav__link">
        Matrix Algebra
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../discrete-maths/" class="md-nav__link">
        Discrete Maths
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../calculus/" class="md-nav__link">
        Calculus
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../complex-analysis/" class="md-nav__link">
        Complex Analysis
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../integral-transforms/" class="md-nav__link">
        Integral Transforms
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../convex-optimization/" class="md-nav__link">
        Convex Optimization
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../3b1b/" class="md-nav__link">
        3Blue1Brown
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../memos/" class="md-nav__link">
        Memos
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
      
      
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
          Languages
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          Languages
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../c/c-primer-plus/" class="md-nav__link">
        C Primer Plus
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../cpp/cpp-notes/" class="md-nav__link">
        C++ Notes
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../python/python-3-object-oriented-programming/" class="md-nav__link">
        Python 3 OOP
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4" >
      
      
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3_4" id="__nav_3_4_label" tabindex="0">
          LaTeX
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_4_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_4">
          <span class="md-nav__icon md-icon"></span>
          LaTeX
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../latex/latex-cheatsheet/" class="md-nav__link">
        LaTeX Cheatsheet
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../latex/latex-maths/" class="md-nav__link">
        LaTeX Maths Symbols
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_5" >
      
      
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3_5" id="__nav_3_5_label" tabindex="0">
          Regular Expressions
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_5_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_5">
          <span class="md-nav__icon md-icon"></span>
          Regular Expressions
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../regex/introducing-regular-expressions/" class="md-nav__link">
        Introducing Regular Expressions
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../regex/regular-expressions-cookbook/" class="md-nav__link">
        Regular Expressions Cookbook
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../web/javascript-the-definitive-guide/" class="md-nav__link">
        JavaScript the Definitive Guide
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../web/html-cheatsheet/" class="md-nav__link">
        HTML Cheatsheet
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../web/css-cheatsheet/" class="md-nav__link">
        CSS Cheatsheet
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
      
      
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
          AI
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          AI
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../ai/deep-learning/" class="md-nav__link">
        Deep Learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../ai/machine-learning/" class="md-nav__link">
        Machine Learning
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
      
      
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
          Competitive Programming
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5">
          <span class="md-nav__icon md-icon"></span>
          Competitive Programming
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_1" >
      
      
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_5_1" id="__nav_5_1_label" tabindex="0">
          Mathematics
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_1">
          <span class="md-nav__icon md-icon"></span>
          Mathematics
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../competitive-programming/prime/" class="md-nav__link">
        素数
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../competitive-programming/exgcd/" class="md-nav__link">
        扩展欧几里得
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../competitive-programming/combination/" class="md-nav__link">
        组合数
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../competitive-programming/binexp/" class="md-nav__link">
        快速幂
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../competitive-programming/fibonacci/" class="md-nav__link">
        类斐波那契数列
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../competitive-programming/gcd-inverse/" class="md-nav__link">
        GCD Inverse
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../competitive-programming/lagrange-interpolation/" class="md-nav__link">
        拉格朗日插值
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_2" >
      
      
        
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_5_2" id="__nav_5_2_label" tabindex="0">
          Data Structures
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_2">
          <span class="md-nav__icon md-icon"></span>
          Data Structures
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../competitive-programming/fenwick/" class="md-nav__link">
        树状数组
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../competitive-programming/segment-tree/" class="md-nav__link">
        线段树
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../competitive-programming/heap/" class="md-nav__link">
        堆
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../competitive-programming/two-heaps/" class="md-nav__link">
        对顶堆
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../competitive-programming/sparse-table/" class="md-nav__link">
        ST 表
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../competitive-programming/ufds/" class="md-nav__link">
        并查集
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_3" >
      
      
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_5_3" id="__nav_5_3_label" tabindex="0">
          Dynamic Programming
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_3">
          <span class="md-nav__icon md-icon"></span>
          Dynamic Programming
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../competitive-programming/intersection-points/" class="md-nav__link">
        直线交点数
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../competitive-programming/lis/" class="md-nav__link">
        最长上升序列
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../competitive-programming/number-of-common-substrings/" class="md-nav__link">
        公共子序列
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_4" >
      
      
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_5_4" id="__nav_5_4_label" tabindex="0">
          Basics
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_4_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_4">
          <span class="md-nav__icon md-icon"></span>
          Basics
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../competitive-programming/quicksort/" class="md-nav__link">
        快速排序 / 第 k 小的数
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../competitive-programming/nearest-point-pair/" class="md-nav__link">
        平面最近点对
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../competitive-programming/permutation-combination-subset/" class="md-nav__link">
        排列、组合、子集问题与回溯
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../competitive-programming/reverse-linked-list/" class="md-nav__link">
        反转链表
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_5" >
      
      
        
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_5_5" id="__nav_5_5_label" tabindex="0">
          Misc
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_5_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_5">
          <span class="md-nav__icon md-icon"></span>
          Misc
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../competitive-programming/josephus-survivor/" class="md-nav__link">
        Josephus Survivor
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../competitive-programming/chopsticks/" class="md-nav__link">
        找筷子
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../competitive-programming/double-cola/" class="md-nav__link">
        Double Cola
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../competitive-programming/alphabetical-coding/" class="md-nav__link">
        字母计数
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../competitive-programming/number-of-trailing-zeros-of-n%21/" class="md-nav__link">
        N! 末尾零数
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../competitive-programming/coordinate-compression/" class="md-nav__link">
        坐标压缩
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
      
      
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
          CS
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_6">
          <span class="md-nav__icon md-icon"></span>
          CS
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../computer-organization/computer-organization/" class="md-nav__link">
        Computer Organization
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../computer-programming/the-practice-of-programming/" class="md-nav__link">
        The Practice of Programming
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" >
      
      
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
          Guides
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_7">
          <span class="md-nav__icon md-icon"></span>
          Guides
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7_1" >
      
      
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_7_1" id="__nav_7_1_label" tabindex="0">
          Linux
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_7_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_7_1">
          <span class="md-nav__icon md-icon"></span>
          Linux
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../guides/linux/linux-initial-setup/" class="md-nav__link">
        Linux Initial Setup
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../guides/linux/ubuntu-beautify/" class="md-nav__link">
        Ubuntu Beautify
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../guides/linux/wsl-initial-setup/" class="md-nav__link">
        WSL Initial Setup
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7_2" >
      
      
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_7_2" id="__nav_7_2_label" tabindex="0">
          Linux Tools
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_7_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_7_2">
          <span class="md-nav__icon md-icon"></span>
          Linux Tools
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../guides/linux-tools/tmux/" class="md-nav__link">
        Tmux
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../guides/linux-tools/vim/" class="md-nav__link">
        Vim
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7_3" >
      
      
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_7_3" id="__nav_7_3_label" tabindex="0">
          Networking
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_7_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_7_3">
          <span class="md-nav__icon md-icon"></span>
          Networking
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../guides/networking/remote-access-and-port-forwarding/" class="md-nav__link">
        Remote Access and Port Forwarding
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../guides/networking/ssh-through-nat/" class="md-nav__link">
        SSH through NAT
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_8" >
      
      
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_8" id="__nav_8_label" tabindex="0">
          Misc
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_8_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_8">
          <span class="md-nav__icon md-icon"></span>
          Misc
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_8_1" >
      
      
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_8_1" id="__nav_8_1_label" tabindex="0">
          Electromagnetics
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_8_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_8_1">
          <span class="md-nav__icon md-icon"></span>
          Electromagnetics
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../electromagnetics/electromagnetics/" class="md-nav__link">
        Electromagnetics
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_8_1_2" >
      
      
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_8_1_2" id="__nav_8_1_2_label" tabindex="0">
          Problemsets
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_8_1_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_8_1_2">
          <span class="md-nav__icon md-icon"></span>
          Problemsets
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../electromagnetics/electric-field/" class="md-nav__link">
        Electric Field
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../electromagnetics/magnetic-field/" class="md-nav__link">
        Magnetic Field
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../electromagnetics/induction/" class="md-nav__link">
        Induction
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../electromagnetics/em-wave/" class="md-nav__link">
        EM Wave
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../control-theory/classical-control/" class="md-nav__link">
        Classical Control Theory
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_8_3" >
      
      
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_8_3" id="__nav_8_3_label" tabindex="0">
          Reading Notes
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_8_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_8_3">
          <span class="md-nav__icon md-icon"></span>
          Reading Notes
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../reading-notes/%E5%A6%82%E4%BD%95%E9%98%85%E8%AF%BB%E4%B8%80%E6%9C%AC%E4%B9%A6/" class="md-nav__link">
        如何阅读一本书
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../reading-notes/%E4%B8%AD%E5%9B%BD%E5%93%B2%E5%AD%A6%E7%AE%80%E5%8F%B2/" class="md-nav__link">
        中国哲学简史
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../reading-notes/%E6%83%8A%E4%BA%BA%E7%9A%84%E5%81%87%E8%AF%B4/" class="md-nav__link">
        惊人的假说
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../reading-notes/%E6%9C%AA%E6%9D%A5%E7%AE%80%E5%8F%B2%EF%BC%9A%E4%BB%8E%E6%99%BA%E4%BA%BA%E5%88%B0%E6%99%BA%E7%A5%9E/" class="md-nav__link">
        未来简史
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_9" >
      
      
        
          
            
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        
        
        <div class="md-nav__link md-nav__link--index ">
          <a href="../../scribbles/">Scribbles</a>
          
            <label for="__nav_9">
              <span class="md-nav__icon md-icon"></span>
            </label>
          
        </div>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_9_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_9">
          <span class="md-nav__icon md-icon"></span>
          Scribbles
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_9_2" >
      
      
        
          
        
      
      
        <label class="md-nav__link" for="__nav_9_2" id="__nav_9_2_label" tabindex="0">
          Automobile
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_9_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_9_2">
          <span class="md-nav__icon md-icon"></span>
          Automobile
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../scribbles/automobile/wheel-alignment/" class="md-nav__link">
        Wheel Alignment
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_9_3" >
      
      
        
          
        
      
      
        <label class="md-nav__link" for="__nav_9_3" id="__nav_9_3_label" tabindex="0">
          CUDA
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_9_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_9_3">
          <span class="md-nav__icon md-icon"></span>
          CUDA
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../scribbles/cuda/cuda-basics/" class="md-nav__link">
        CUDA Basics
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_9_4" >
      
      
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_9_4" id="__nav_9_4_label" tabindex="0">
          Maths
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_9_4_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_9_4">
          <span class="md-nav__icon md-icon"></span>
          Maths
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../scribbles/maths/jacobian-matrix/" class="md-nav__link">
        Jacobian Matrix
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../scribbles/maths/chebyshev-center/" class="md-nav__link">
        Chebyshev Center
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_9_5" >
      
      
        
          
        
      
      
        <label class="md-nav__link" for="__nav_9_5" id="__nav_9_5_label" tabindex="0">
          Numerical Analysis
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_9_5_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_9_5">
          <span class="md-nav__icon md-icon"></span>
          Numerical Analysis
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../scribbles/numerical-analysis/fast-inv-sqrt/" class="md-nav__link">
        Fast Inverse Square Root, 0x5F3759DF
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_9_6" >
      
      
        
          
        
      
      
        <label class="md-nav__link" for="__nav_9_6" id="__nav_9_6_label" tabindex="0">
          OpenCV
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_9_6_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_9_6">
          <span class="md-nav__icon md-icon"></span>
          OpenCV
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../scribbles/opencv/opencv-basics/" class="md-nav__link">
        OpenCV Basics
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_9_7" >
      
      
        
          
        
      
      
        <label class="md-nav__link" for="__nav_9_7" id="__nav_9_7_label" tabindex="0">
          Python
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_9_7_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_9_7">
          <span class="md-nav__icon md-icon"></span>
          Python
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../scribbles/python/python-type-hints/" class="md-nav__link">
        Python Type Hints
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_9_8" >
      
      
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_9_8" id="__nav_9_8_label" tabindex="0">
          PyTorch
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_9_8_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_9_8">
          <span class="md-nav__icon md-icon"></span>
          PyTorch
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../scribbles/pytorch/pytorch-gpu-management/" class="md-nav__link">
        PyTorch GPU Management
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../scribbles/pytorch/pytorch-loss-functions/" class="md-nav__link">
        PyTorch Loss Functions
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../scribbles/pytorch/broadcasting/" class="md-nav__link">
        Broadcasting
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#ch-2-entropy-relative-entropy-mutual-information" class="md-nav__link">
    Ch. 2: Entropy, relative entropy, mutual information
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ch-3-asymptotic-equipartition-property" class="md-nav__link">
    Ch. 3: Asymptotic equipartition property
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ch-4-entropy-rates-of-a-stochastic-process" class="md-nav__link">
    Ch. 4: Entropy rates of a stochastic process
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ch-5-data-compression" class="md-nav__link">
    Ch. 5: Data compression
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ch-7-channel-capacity" class="md-nav__link">
    Ch. 7: Channel capacity
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ch-8-differential-entropy" class="md-nav__link">
    Ch. 8: Differential entropy
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ch-9-gaussian-channel" class="md-nav__link">
    Ch. 9: Gaussian channel
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ch-10-rate-distortion-theory" class="md-nav__link">
    Ch. 10: Rate distortion theory
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ch-11-information-theory-and-statistics" class="md-nav__link">
    Ch. 11: Information theory and statistics
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  
  




<h1 id="elements-of-information-theory">Elements of Information Theory<a class="headerlink" href="#elements-of-information-theory" title="Permanent link">&para;</a></h1>
<blockquote>
<p><em>Second Edition</em><br />
<em>Thomas M. Cover, Joy A. Thomas</em><br />
<em>2006, John Wiley &amp; Sons</em>  </p>
</blockquote>
<hr />
<h2 id="ch-2-entropy-relative-entropy-mutual-information">Ch. 2: Entropy, relative entropy, mutual information<a class="headerlink" href="#ch-2-entropy-relative-entropy-mutual-information" title="Permanent link">&para;</a></h2>
<p><strong>Entropy</strong> measures uncertainty of a discrete random variable. Let <span class="arithmatex">\(X\)</span> be a discrete r.v. with alphabet <span class="arithmatex">\(\mathcal{X} = \{x: \Pr(X=x)&gt;0\}\)</span> and probability mass function <span class="arithmatex">\(p(x) = \Pr(X=x)\)</span>. The entropy of <span class="arithmatex">\(X\)</span> is</p>
<p class="arithmatex">\[
H(X) = \sum_{x \in \mathcal{X}} p(x)\log\frac{1}{p(x)} = \operatorname{E}_{p}\left[\log\frac{1}{p(X)}\right],
\]</p>
<p>where <span class="arithmatex">\(\log\)</span> denotes logarithm base 2 unless specified, thus the entropy value is in bits. Also, if the base of the logarithm is <span class="arithmatex">\(\mathrm{e}\)</span>, the entropy is measured in nats, and entropy of any base can convert to another according to the change of base rule.</p>
<p><strong>Joint entropy</strong>:</p>
<p class="arithmatex">\[
H(X, Y) = \sum_{x\in \mathcal{X}}\sum_{y\in \mathcal{Y}} p(x, y)\log\frac{1}{p(x, y)} = \mathrm{E}_{p(X, Y)}\left[\frac{1}{p(X, Y)}\right].
\]</p>
<p><strong>Conditional entropy</strong>:</p>
<p class="arithmatex">\[
\begin{aligned}
H(Y|X) &amp;= \sum_{x\in\mathcal{X}} p(x)H(Y|X = x) \\
&amp;= \sum_{x\in \mathcal{X}}p(x)\sum_{y\in \mathcal{Y}}p(y|x)\log\frac{1}{p(y|x)}\\
&amp;= \sum_{x\in \mathcal{X}}\sum_{y\in \mathcal{Y}}p(x, y)\log\frac{1}{p(y|x)}\\
&amp;= \mathrm{E}\left[\frac{1}{\log p(Y|X)}\right].
\end{aligned}
\]</p>
<p><strong>Chain rules</strong>:</p>
<p><span class="arithmatex">\(H(X, Y) = H(X) + H(Y|X)\)</span>.</p>
<p><img alt="" src="../img/infoth_2_chainrule1.svg" style="transform: scale(1.2);" /></p>
<p><span class="arithmatex">\(H(X, Y | Z) = H(X | Z) + H(Y | X, Z)\)</span>.</p>
<p><img alt="" src="../img/infoth_2_chainrule2.svg" style="transform: scale(1.2); padding: 1em;" /></p>
<p>Commas have higher priority than conditionings.</p>
<p><strong>Relative entropy (KL Distance)</strong>:</p>
<p class="arithmatex">\[
D(p\|q) = \sum_{x\in \mathcal{X}}p(x)\log\frac{p(x)}{q(x)} = \mathrm{E}_p\left[\log\frac{p(X)}{q(X)}\right].
\]</p>
<p><strong>Mutual information</strong>:</p>
<p class="arithmatex">\[
I(X;Y) = D(p(x, y) \| p(x)p(y)) = \mathrm{E}_{p(X, Y)}\left[\log\frac{p(X, Y)}{p(X)p(Y)}\right].
\]</p>
<p class="arithmatex">\[
\begin{aligned}
I(X; Y) &amp;= H(X) - H(X|Y) \\
&amp;= H(Y) - H(Y|X)\\
&amp;= H(X) + H(Y) - H(X, Y).
\end{aligned}
\]</p>
<p><img alt="" src="../img/infoth_2_i.svg" style="transform: scale(1.2);" /></p>
<p class="arithmatex">\[
I(X; X) = H(X),\quad H(X|X) = 0.
\]</p>
<p><strong>Chain rule for entropy</strong>:</p>
<p class="arithmatex">\[
\begin{aligned}
H(X_1, X_2, \cdots, X_n) &amp;= \sum_{i=1}^n H(X_i | X_{i-1}, \cdots, X_1) \\
&amp;= H(X_1) + H(X_2|X_1) + H(X_3|X_2, X_1), \cdots.
\end{aligned}
\]</p>
<p>It is very obvious graphically.</p>
<p><strong>Conditional mutual information</strong>:</p>
<p class="arithmatex">\[
I(X; Y | Z) = H(X | Z) - H(X | Y, Z) = \mathrm{E}\left[\log\frac{p(X, Y | Z)}{p(X|Z)p(Y| Z)}\right].
\]</p>
<p><img alt="" src="../img/infoth_2_cond_i.svg" style="transform: scale(1.2);" /></p>
<p>Semicolons have higher priority than conditionings.</p>
<p><strong>Chain rule for information</strong>:</p>
<p class="arithmatex">\[
\begin{aligned}
I(X_1, X_2, \cdots, X_n ; Y) &amp;= \sum_{i=1}^n I(X_i; Y | X_{i-1}, X_{i-2}, X_1) \\
&amp;= I(X_1; Y) + I(X_2; Y | X_1) + I(X_3; Y | X_2, X_1), \cdots.
\end{aligned}
\]</p>
<p>It is obvious graphically.</p>
<p><strong>Conditional relative entropy</strong>:</p>
<p class="arithmatex">\[
D(p(y|x)\|q(y|x)) = \mathrm{E}_{p(X, Y)}\left[\log\frac{p(Y|X)}{q(Y|X)}\right].
\]</p>
<p><strong>Chain rule for relative entropy</strong>:</p>
<p class="arithmatex">\[
D(p(x, y)\|q(x, y)) = D(p(x)\|q(x)) + D(p(y|x) \| q(y|x)).
\]</p>
<p><strong>Independence bound on entropy</strong>: <span class="arithmatex">\(H(X_1, X_2, \cdots, X_n)\le \sum H(X_i)\)</span>, with equality iff all <span class="arithmatex">\(X_i\)</span> are independent.</p>
<p><strong>Log sum inequality</strong>:</p>
<p class="arithmatex">\[
\sum a_i \log\frac{a_i}{b_i} \ge \sum a_i \log \frac{\sum a_i}{\sum b_i}.
\]</p>
<p><strong>Relative entropy</strong> <span class="arithmatex">\(D(p\| q)\)</span> is <strong>convex</strong> in the pair <span class="arithmatex">\((p, q)\)</span>.</p>
<p><strong>Entropy</strong> is <strong>concave</strong>, as <span class="arithmatex">\(H(p) = \log|\mathcal{X}| - D(p \| u)\)</span>, where <span class="arithmatex">\(u\)</span> is the uniform distribution on <span class="arithmatex">\(\mathcal{X}\)</span> and <span class="arithmatex">\(\log|\mathcal{X}| = \mathrm{E}\left[\log\frac{1}{u(x)}\right]\)</span>.</p>
<p><strong>Mutual information</strong> <span class="arithmatex">\(I(X; Y)\)</span> is <strong>concave</strong> with respect to <span class="arithmatex">\(p(x)\)</span> for fixed <span class="arithmatex">\(p(y|x)\)</span>; <strong>convex</strong> with respect to <span class="arithmatex">\(p(y|x)\)</span> for fixed <span class="arithmatex">\(p(x)\)</span>.</p>
<p>A <strong>Markov chain</strong> <span class="arithmatex">\(X\rightarrow Y \rightarrow Z\)</span> has the following properties:</p>
<ul>
<li><span class="arithmatex">\(p(x, y, z) = p(x)p(y|x)p(z|y)\)</span> (definition),</li>
<li><span class="arithmatex">\(p(z|y) = p(z|x, y)\)</span>,</li>
<li><span class="arithmatex">\(p(x, z | y) = p(x|y)p(z|y)\)</span>,</li>
<li><span class="arithmatex">\(Z \rightarrow Y \rightarrow X\)</span> (<span class="arithmatex">\(X\leftrightarrow Y \leftrightarrow Z\)</span>),</li>
<li>if <span class="arithmatex">\(Z = f(Y)\)</span>, <span class="arithmatex">\(X \rightarrow Y \rightarrow Z\)</span>,</li>
<li><span class="arithmatex">\(I(X; Z|Y) = 0\)</span>.</li>
</ul>
<p><strong>Data processing inequality</strong>: in a Markov chain <span class="arithmatex">\(X\rightarrow Y \rightarrow Z\)</span>, <span class="arithmatex">\(I(X; Y) \ge I(X; Z)\)</span>. Also, observation of <span class="arithmatex">\(Z\)</span> reduces dependence between <span class="arithmatex">\(X\)</span> and <span class="arithmatex">\(Y\)</span>: <span class="arithmatex">\(I(X; Y | Z) \le I(X; Y)\)</span>, which might be the opposite when they do not form a Markov chain: <span class="arithmatex">\(X\)</span> and <span class="arithmatex">\(Y\)</span> independently observes Bernoulli distribution with <span class="arithmatex">\(p=0.5\)</span>, and <span class="arithmatex">\(Z = X + Y\)</span>.</p>
<p><strong>Fano's inequality</strong>: <span class="arithmatex">\(Y\)</span> is related to <span class="arithmatex">\(X\)</span> by <span class="arithmatex">\(p(y|x)\)</span>, and an estimator of <span class="arithmatex">\(Y\)</span> is <span class="arithmatex">\(g(Y) = X\)</span>. Therefore, <span class="arithmatex">\(X \rightarrow Y \rightarrow \hat{X}\)</span>. The probability of error is defined as <span class="arithmatex">\(P_e = \Pr\{X\neq \hat{X}\}\)</span>. Fano's inequality is</p>
<p class="arithmatex">\[
H(P_e) + P_e \log |\mathcal{X}| \ge H(X|\hat{X}) \ge H(X|Y).
\]</p>
<div class="admonition info">
<p class="admonition-title">Notation <span class="arithmatex">\(H(p)\)</span></p>
<p>Since a random variable is essentially a probability distribution. Entropy of a r.v. is the entropy of this distribution. That is, <span class="arithmatex">\(H(p_1, p_2, \cdots, p_k) = -\sum_{i=1}^k p_i\log p_i\)</span>. Furthermore, <span class="arithmatex">\(H(p)\)</span> denotes the entropy of an r.v. that follows the distribution <span class="arithmatex">\(\{p, 1-p\}\)</span>.</p>
</div>
<div class="admonition abstract">
<p class="admonition-title">Proof</p>
<p class="arithmatex">\[
\begin{aligned}
H(E, X|\hat{X})
&amp;= H(X|\hat{X}) + H(E|X, \hat{X}) \\
&amp;= H(X|\hat{X}) \\
&amp;= H(E|\hat{X}) + H(X | E, \hat{X}) \\
&amp;\le H(E) + H(X | E, \hat{X}),
\end{aligned}
\]</p>
<p>where</p>
<p class="arithmatex">\[
\begin{aligned}
H(X|E, \hat{X})
&amp;= H((X | \hat{X}) | E) \\
&amp;= P(E=1)H(X|\hat{X}) + P(E=0)H(X|\hat{X}) \\
&amp;= P(E=1)H(X|\hat{X}) \\
&amp;= P_eH(X|\hat{X}) \\
&amp;\le P_e H(X) \\
&amp;\le P_e \log |\mathcal{X}|.
\end{aligned}
\]</p>
<p>If <span class="arithmatex">\(\hat{X}\in \mathcal{X}\)</span>, since <span class="arithmatex">\(X \neq \hat{X}\)</span> in the <span class="arithmatex">\(E=1\)</span> case, <span class="arithmatex">\(H(X|E, \hat{X}) \le P_e \log (|\mathcal{X}|-1)\)</span>, and Fano's inequality changes accordingly.</p>
</div>
<p>If <span class="arithmatex">\(X\)</span> and <span class="arithmatex">\(X'\)</span> are i.i.d. with entropy <span class="arithmatex">\(H(X)\)</span>,</p>
<p class="arithmatex">\[
\Pr(X = X') \ge 2^{-H(X)},
\]</p>
<p>with equality iff uniformly distributed.</p>
<div class="admonition abstract">
<p class="admonition-title">Proof</p>
<p class="arithmatex">\[
\Pr(X = X') = \sum p^2(x) = \sum p(x)2^{\log p(x)} = \operatorname{E}2^{\log p(x)} \overset{\text{convex}}{\ge} 2^{\operatorname{E}\log p(x)} = 2^{-H(x)}.
\]</p>
</div>
<p>If <span class="arithmatex">\(X \sim p(x)\)</span>, <span class="arithmatex">\(X' \sim r(x)\)</span> independently,</p>
<p class="arithmatex">\[
\Pr(X = X') \ge 2^{-H(p) - D(p \| r)}, \\
\Pr(X = X') \ge 2^{-H(r) - D(r \| p)}. \\
\]</p>
<h2 id="ch-3-asymptotic-equipartition-property">Ch. 3: Asymptotic equipartition property<a class="headerlink" href="#ch-3-asymptotic-equipartition-property" title="Permanent link">&para;</a></h2>
<p><strong>Convergence of random variables</strong>: <span class="arithmatex">\(\{X_n\} \rightarrow X\)</span> is said to be</p>
<ul>
<li>in probability, if <span class="arithmatex">\(\forall \epsilon &gt; 0\)</span>, <span class="arithmatex">\(\Pr\{|X_n - X| &gt; \epsilon\}\)</span>,</li>
<li>in mean square, if <span class="arithmatex">\(\operatorname{E}(X_n - X)^2 \rightarrow 0\)</span>,</li>
<li>with probability 1, if <span class="arithmatex">\(\Pr\left\{\lim\limits_{n\rightarrow \infty} X_n = X\right\} = 1\)</span>.</li>
</ul>
<p><strong>Asymptotic equipartition property</strong>: if <span class="arithmatex">\(X_1, X_2, \cdots, X_n\)</span> are i.i.d. <span class="arithmatex">\(\sim p(x)\)</span>,</p>
<p class="arithmatex">\[
-\frac{1}{n} \log p(X_1, X_2, \cdots, X_n) \rightarrow H(X) \quad \text{in probability.}
\]</p>
<div class="admonition abstract">
<p class="admonition-title">Proof</p>
<p>A few derivations lead to</p>
<p class="arithmatex">\[
-\frac{1}{n}\sum\log p(X_i) \rightarrow -\operatorname{E}\log p(X),
\]</p>
<p>and the law of large numbers finishes the proof.</p>
</div>
<p><span class="arithmatex">\(A_\epsilon^{(n)}\)</span> is the <strong>typical set</strong> with respect to <span class="arithmatex">\(p(x)\)</span> iff every element <span class="arithmatex">\((x_1, x_2, \cdots, x_n)\in \mathcal{X}^n\)</span>, and</p>
<p class="arithmatex">\[
2^{-n(H(X) + \epsilon)} \le p(x_1, x_2, \cdots, x_n) \le 2^{-n(H(X) - \epsilon)}.
\]</p>
<p>The equation above is equivalent to</p>
<p class="arithmatex">\[
\left|-\frac{1}{n}\log p(x_1, x_2, \cdots, x_n) - H(X)\right| \le \epsilon.
\]</p>
<p>According to AEP, <span class="arithmatex">\(\forall\delta &gt; 0\)</span>, <span class="arithmatex">\(\exists n_0\in\mathbb{R}\)</span>, <span class="arithmatex">\(\forall n \ge n_0\)</span>, <span class="arithmatex">\(\Pr\left\{\left|-\frac{1}{n}\log p(x_1, x_2, \cdots, x_n) - H(X)\right|&lt; \epsilon\right\} &gt; 1 - \delta\)</span>. We can set <span class="arithmatex">\(\delta\)</span> to <span class="arithmatex">\(\epsilon\)</span>.</p>
<p class="arithmatex">\[
2^{-n(H(X) + \epsilon)}\left|A_\epsilon^{(n)}\right| \le \sum_{A_\epsilon^{(n)}} p(x_1, x_2, \cdots, x_n) \le \sum_{\mathcal{X}^n} p(x_1, x_2, \cdots, x_n) = 1.
\]</p>
<p>For <span class="arithmatex">\(n\)</span> sufficiently large,</p>
<p class="arithmatex">\[
2^{-n(H(X) - \epsilon)}\left|A_\epsilon^{(n)}\right| \ge \sum_{A_\epsilon^{(n)}} p(x_1, x_2, \cdots, x_n) \ge \Pr\{A_\epsilon^{(n)}\} &gt; 1 - \epsilon.
\]</p>
<p>Therefore, indexing elements in the typical set <span class="arithmatex">\(A_\epsilon^{(n)}\)</span> requires no more than <span class="arithmatex">\(n(H + \epsilon) + 1\)</span> bits, and indexing elements in <span class="arithmatex">\(A_\epsilon^{(n)\mathrm{C}}\)</span> requires no more than <span class="arithmatex">\(n\log |\mathcal{X}| + 1\)</span> bits. To index all the elements in <span class="arithmatex">\(\mathcal{X}^n\)</span>, there is an additional bit to indicate whether in the typical set.</p>
<p class="arithmatex">\[
\begin{aligned}
\operatorname{E}l(X^n)
&amp;= \sum_{A_\epsilon^{(n)}}p(x^n)l(x^n) + \sum_{A_\epsilon^{(n)\mathrm{C}}}p(x^n)l(x^n) \\
&amp;\le \Pr\{A_\epsilon^{(n)}\}(n(H + \epsilon) + 2) + \Pr\{A_\epsilon^{(n)\mathrm{C}}\}(n\log|\mathcal{X}| + 2) \\
&amp;\le n(H+\epsilon) + 2 + \delta(n\log|\mathcal{X}| + 2) \\
&amp;= n\left(H + \epsilon + \frac{2}{n} + \delta\log|\mathcal{X}| + \frac{2}{n}\right) \\
&amp;= n(H + \epsilon').
\end{aligned}
\]</p>
<p>For <span class="arithmatex">\(X^n\)</span> i.i.d. <span class="arithmatex">\(\sim p(x)\)</span>, <span class="arithmatex">\(\operatorname{E}\left[\frac{1}{n}l(x^n)\right] \le H(X) + \epsilon\)</span>. We can encode <span class="arithmatex">\(X^n\)</span> using <span class="arithmatex">\(nH(X)\)</span> bits on average.</p>
<p>A <strong>high probability set</strong> <span class="arithmatex">\(B_\delta^{(n)} \subset \mathcal{X}^n\)</span> is the smallest set with <span class="arithmatex">\(\Pr(B_\delta^{(n)}) \ge 1 - \delta\)</span>. <span class="arithmatex">\(B_\delta^{(n)}\)</span> must have at least <span class="arithmatex">\(2^{nH}\)</span> elements.</p>
<h2 id="ch-4-entropy-rates-of-a-stochastic-process">Ch. 4: Entropy rates of a stochastic process<a class="headerlink" href="#ch-4-entropy-rates-of-a-stochastic-process" title="Permanent link">&para;</a></h2>
<p>A stochastic process <span class="arithmatex">\(\{X_i\}\)</span> is said to be <strong>stationary</strong> if</p>
<p class="arithmatex">\[
\begin{aligned}
&amp;\Pr(X_1 = x_1, X_2 = x_2, \cdots, X_n = x_n) \\
= &amp;\Pr(X_{1+l} = x_1, X_{2+l} = x_2, \cdots, X_{n+l} = x_n).
\end{aligned}
\]</p>
<p>A discrete stationary process is said to be a <strong>Markov process</strong> (<strong>Markov chain</strong>) if</p>
<p class="arithmatex">\[
\begin{aligned}
&amp;\Pr(X_{n+1} = x_{n+1} \mid X_n = x_n, \cdots, X_1 = x_1) \\
= &amp;\Pr(X_{n+1} = x_{n+1} \mid X_n = x_n).
\end{aligned}
\]</p>
<p>And the joint distribution for a Markov process</p>
<p class="arithmatex">\[
p(x_1, x_2, \cdots, x_n) = p(x_1)p(x_2|x_1)p(x_3|x_2)\cdots p(x_n|x_{n-1}).
\]</p>
<p>A Markov process is said to be <strong>time invariant</strong> if for any <span class="arithmatex">\(n\)</span>,</p>
<p class="arithmatex">\[
\Pr(X_{n+1}=b \mid X_n = a) = \Pr(X_2 = b \mid X_1 = a).
\]</p>
<p>We always assume a Markov process is time invariant unless otherwise stated.</p>
<p>A Markov process that can go from any state to any other state with positive probability in a finite number of steps is said to be <strong>irreducible</strong>.</p>
<p>A Markov process that for every state, lengths of all loops it sits on has the greatest common divisor 1, is said to be <strong>aperiodic</strong>.</p>
<p>A Markov process at time <span class="arithmatex">\(n+1\)</span> has distribution <span class="arithmatex">\(p(x_{n+1}) = \sum\limits_{x_n}p(x_n)P_{x_n x_{n+1}}\)</span>. The distribution such that at time <span class="arithmatex">\(n\)</span> and <span class="arithmatex">\(n+1\)</span> are equal is called a <strong>stationary distribution</strong>. If the initial state of a Markov process is drawn from a stationary distribution, the Markov process forms a stationary process.</p>
<div class="admonition note">
<p class="admonition-title">Example</p>
<p>Derive the stationary distribution for a Markov process with transition matrix</p>
<p class="arithmatex">\[
P = \begin{bmatrix} 1-\alpha &amp; \alpha \\ \beta &amp; 1-\beta \end{bmatrix}.
\]</p>
<p>Solution: Let the stationary distribution be <span class="arithmatex">\((\mu_1, \mu_2)\)</span>. The distribution at time 1 and 2 are</p>
<p class="arithmatex">\[
\begin{cases}
\Pr(X_1 = 1) = \mu_1, \\
\Pr(X_1 = 1) = \mu_2,
\end{cases}\quad
\begin{cases}
\Pr(X_2 = 1) = \mu_1(1-\alpha) + \mu_2\beta, \\
\Pr(X_2 = 1) = \mu_1\alpha + \mu_2(1-\beta),
\end{cases}
\]</p>
<p>Let those 2 distributions be equal, we obtain <span class="arithmatex">\(\alpha \mu_1 = \beta \mu_2\)</span>. Since <span class="arithmatex">\(\mu_1 + \mu_2 = 1\)</span>, the stationary distribution is</p>
<p class="arithmatex">\[
\mu_1 = \frac{\beta}{\alpha + \beta}, \quad \mu_2 = \frac{\alpha}{\alpha + \beta}.
\]</p>
</div>
<p>For an irreducible and aperiodic finite-state Markov chain, its stationary distribution is unique, and from any starting distribution, the distribution at time <span class="arithmatex">\(t\)</span> tends to be stationary as <span class="arithmatex">\(t \rightarrow \infty\)</span>.</p>
<p><strong>Entropy rate</strong>, as per symbol entropy on average:</p>
<p class="arithmatex">\[
H(\mathcal{X}) = \lim_{n\rightarrow\infty} \frac{1}{n}H(X_1, X_2, \cdots, X_n).
\]</p>
<p>If <span class="arithmatex">\(X_1, X_2, \cdots, X_n\)</span> are i.i.d., <span class="arithmatex">\(H(\mathcal{X}) = H(X_i)\)</span>.</p>
<p>If <span class="arithmatex">\(X_1, X_2, \cdots, X_n\)</span> are independent, <span class="arithmatex">\(H(X_1, X_2, \cdots, X_n) = \sum H(X_i)\)</span>, but the existence of limit need to be confirmed.</p>
<div class="admonition abstract">
<p class="admonition-title">Proof</p>
<p>For simplicity, <span class="arithmatex">\(x_1, x_2, \cdots, x_n\)</span> is denoted by <span class="arithmatex">\(x^n\)</span>, and <span class="arithmatex">\(\sum\limits_{x_i \in \mathcal{X}_i}\)</span> is denoted by <span class="arithmatex">\(\sum\limits_{x_i}\)</span>.</p>
<p class="arithmatex">\[
\begin{aligned}
H(X_1, X_2, \cdots, X_{n+1}) &amp;= \sum_{x^{n+1}}p(x^{n+1})\log\frac{1}{p(x^{n+1})} \\
\text{\footnotesize(seperate $x_{n+1}$, independence)} &amp;= \sum_{x_{n+1}} \sum_{x^n} p(x^n)p(x_{n+1})\log\frac{1}{p(x^n)p(x_{n+1})} \\
&amp;= \sum_{x_{n+1}}p(x_{n+1})\sum_{x^n}p(x^n)\log\frac{1}{p(x^n)} + \sum_{x_{n+1}}p(x_{n+1})\sum_{x^n}p(x^n)\log\frac{1}{p(x_{n+1})} \\
&amp;= \sum_{x^n}p(x^n)\log\frac{1}{p(x^n)} + \sum_{x_{n+1}}p(x_{n+1})\log\frac{1}{p(x_{n+1})} \\
&amp;= H(X_1, X_2, \cdots, X_n) + H(X_{n+1}).
\end{aligned}
\]</p>
</div>
<p><strong>Entropy rate</strong>, as conditional entropy of the last symbol given history:</p>
<p class="arithmatex">\[
H'(\mathcal{X}) = \lim_{n\rightarrow\infty} H(X_n \mid X_{n-1}, X_{n-2}, \cdots, X_1).
\]</p>
<p>For a stationary process, <span class="arithmatex">\(H(\mathcal{X}) = H'(\mathcal{X})\)</span>, and <span class="arithmatex">\(H'(X)\)</span> is non-increasing as <span class="arithmatex">\(H(X_{n+1} \mid X_n, \cdots, X_1) \le H(X_{n+1} \mid X_n, \cdots, X_2) = H(X_n \mid X_{n-1}, \cdots, X_1)\)</span>.</p>
<p><strong>Cesaro mean</strong>: If <span class="arithmatex">\(a_n \rightarrow a\)</span> and <span class="arithmatex">\(b_n = \dfrac{1}{n}\sum\limits_{i=1}^n a_i\)</span>, <span class="arithmatex">\(b_n \rightarrow a\)</span>.</p>
<p>For a stationary Markov process, <span class="arithmatex">\(H'(\mathcal{X}) = H'(\mathcal{X}) = \lim H(X_n | X_{n-1}) = H(X_2 | X_1)\)</span>.</p>
<p>The entropy rate of a stationary Markov chain with transition matrix <span class="arithmatex">\(P\)</span>, and stationary distribution <span class="arithmatex">\(\mu\)</span>, the entropy rate is</p>
<p class="arithmatex">\[
H(\mathcal{X}) = H(X_2|X_1) = \sum_i \mu_i H(X_2 | X_1 = x_i) = \sum_i \mu_i\left(\sum_j P_{ij} \log\frac{1}{P_{ij}}\right).
\]</p>
<h2 id="ch-5-data-compression">Ch. 5: Data compression<a class="headerlink" href="#ch-5-data-compression" title="Permanent link">&para;</a></h2>
<p>A <strong>nonsingular code</strong> is a code of which every element maps to a different code. <span class="arithmatex">\(x \neq x' \Rightarrow C(x) \neq C(x')\)</span>.</p>
<p>The extension of a code <span class="arithmatex">\(C\)</span> is the concatenation of codewords: <span class="arithmatex">\(C(x_1x_2\cdots x_n) = C(x_1)C(x_2)\cdots C(x_n)\)</span>.</p>
<p>A <strong>uniquely decodable code</strong> is a code that has nonsingular extension.</p>
<p>A <strong>prefix code</strong> or an <strong>instantaneous code</strong> is a code of which no codeword is a prefix of any other prefix of any other codeword (can be decoded without reference to future codewords, self-punctuating).</p>
<p>All <span class="arithmatex">\(\supset\)</span> nonsingular <span class="arithmatex">\(\supset\)</span> uniquely decodable <span class="arithmatex">\(\supset\)</span> instantaneous.</p>
<p><strong>Kraft inequality</strong>: for any instantaneous code over an alphabet of size <span class="arithmatex">\(D\)</span>, the codeword lengths <span class="arithmatex">\(l_1, l_2, \cdots\)</span> satisfies</p>
<p class="arithmatex">\[
\sum_i D^{-l_i} \le 1.
\]</p>
<p>Conversely, from a set of codewords lengths that satisfy Kraft inequality, one can construct an instantaneous code.</p>
<p><strong>Extended Kraft inequality</strong>: for any prefix code that has countably infinite codewords with length <span class="arithmatex">\(l_1, l_2, \cdots\)</span>,</p>
<p class="arithmatex">\[
\sum_{i=1}^\infty D^{-l_i} \le 1.
\]</p>
<p>To find the <strong>optimal code lengths</strong>, that is</p>
<p class="arithmatex">\[
\begin{aligned}
\operatorname{minimize~~}&amp; L = \sum p_i l_i \\
\operatorname{s.t.~~}&amp; \sum_i D^{-l_i} \le 1, \\
&amp; l_i \in \mathbb{Z}^+.
\end{aligned}
\]</p>
<p>Removing the integer constraint leads to results</p>
<p class="arithmatex">\[
l_i^* = \log_D p_i, \quad L^* = H_D(X).
\]</p>
<p>The <strong>expected length</strong> <span class="arithmatex">\(L\)</span> of any prefix <span class="arithmatex">\(D\)</span>-ary code</p>
<p class="arithmatex">\[
L \ge H_D(X),
\]</p>
<p>with equality iff <span class="arithmatex">\(D^{-l_i} = p_i\)</span> for every symbol, that is, each of the probability equals to <span class="arithmatex">\(D^{-n}\)</span> for some integer <span class="arithmatex">\(n\)</span>, and in this case the distribution is said to be <strong><span class="arithmatex">\(D\)</span>-adic</strong>.</p>
<p>When the distribution is not <span class="arithmatex">\(D\)</span>-adic, we choose <span class="arithmatex">\(l_i = \left\lceil\log_D \dfrac{1}{p_i}\right\rceil\)</span>, and that means <span class="arithmatex">\(\log_D \dfrac{1}{p_i} \le l_i &lt; \log_D \dfrac{1}{p_i}+1\)</span>, finally</p>
<p class="arithmatex">\[
H_D(X) \le L &lt; H_D(X) + 1.
\]</p>
<p>For a sequence of symbols <span class="arithmatex">\((x_1, x_2, \cdots, x_n) \in \mathcal{X}^n\)</span>,</p>
<p class="arithmatex">\[
H(X_1, X_2, \cdots, X_n) \le \operatorname{E}l(X_1, X_2, \cdots, X_n) &lt; H(X_1, X_2, \cdots, X_n) + 1.
\]</p>
<p>therefore <strong>the expected per symbol length</strong> <span class="arithmatex">\(L_n\)</span> satisfies</p>
<p class="arithmatex">\[
\frac{H(X_1, X_2, \cdots, X_n)}{n} \le L_n^* &lt; \frac{H(X_1, X_2, \cdots, X_n)}{n} + \frac{1}{n}.
\]</p>
<p>If <span class="arithmatex">\(X_1, X_2, \cdots X_n\)</span> is a stationary stochastic process, <span class="arithmatex">\(L_n^* \rightarrow H(\mathcal{X})\)</span>. The expected number of bits required per symbol to describe the process is the entropy rate.</p>
<p>If <span class="arithmatex">\(X_i\)</span> are i.i.d., <span class="arithmatex">\(H(\mathcal{X}) \le L_n &lt; H(\mathcal{X}) + \dfrac{1}{n}\)</span>.</p>
<p>If we estimate the distribution of <span class="arithmatex">\(X\)</span> to be <span class="arithmatex">\(q(x)\)</span>, but the true distribution is <span class="arithmatex">\(p(x)\)</span>, the code assignment <span class="arithmatex">\(l(x) = \left\lceil\log\frac{1}{q(x)}\right\rceil\)</span>, the penalty for expected length is the relative entropy:</p>
<p class="arithmatex">\[
H(p) + D(p\|q) \le \operatorname{E}_p l(X) &lt; H(p) + D(p\|q) + 1.
\]</p>
<p><strong>McMillan theorem</strong>: any uniquely decodable <span class="arithmatex">\(D\)</span>-ary code also satisfies Kraft inequality, and the converse holds too.</p>
<p><strong>Shannon code</strong> assigns a symbol of probability <span class="arithmatex">\(p_i\)</span> with codeword length <span class="arithmatex">\(\left\lceil\log\frac{1}{p_i}\right\rceil\)</span>, but it is not necessarily optimal.</p>
<div class="admonition note">
<p class="admonition-title">Example</p>
<p>Two symbols with probabilities <span class="arithmatex">\(\{0.0001, 0.9999\}\)</span>, an optimal code requires 1 codeword of length 1, but Shannon code obviously does not.</p>
</div>
<p>An optimal code does not necessarily always have codeword lengths less than <span class="arithmatex">\(\left\lceil\log\frac{1}{p_i}\right\rceil\)</span>.</p>
<div class="admonition note">
<p class="admonition-title">Example</p>
<p>Huffman code for distribution <span class="arithmatex">\(\{\frac{1}{3}, \frac{1}{3}, \frac{1}{4}, \frac{1}{12}\}\)</span> has codeword lengths (2, 2, 2, 2) or (1, 2, 3, 3) (yes, not unique), while <span class="arithmatex">\(3 &gt; \log\left(\frac{1}{4}\right)^{-1}\)</span>.</p>
</div>
<p><strong>Fano code</strong> orders symbols in decreasing order of probabilities, and each time divide the set into almost equal parts. It is suboptimal.</p>
<p><span class="arithmatex">\(F(x)\)</span> denotes the cumulative distribution function, and a modified one <span class="arithmatex">\(\bar{F}(x) = \sum_{a&lt;x}p(a) + \frac{1}{2}p(x)\)</span>, is the probability of less than middle of <span class="arithmatex">\(x\)</span>. Write <span class="arithmatex">\(\bar{F}(x)\)</span> in binary and truncate to <span class="arithmatex">\(\left\lceil\log\frac{1}{p(x)}\right\rceil + 1\)</span> after decimal. This prefix code is called <strong>Shannon-Fano-Elias code</strong> and has expected length <span class="arithmatex">\(L &lt; H(X) + 2\)</span></p>
<p><strong>Competitive optimality of Shannon code</strong> (<span class="arithmatex">\(l(x) = \left\lceil\log\frac{1}{p(x)}\right\rceil\)</span>): for any uniquely decodable code with codeword lengths <span class="arithmatex">\(l'(x)\)</span>,</p>
<p class="arithmatex">\[
\Pr(l(x) \ge l'(x) + c) \le \frac{1}{2^{c-1}},
\]</p>
<p class="arithmatex">\[
\Pr(l(x) &lt; l'(x)) \ge \Pr(l(x) &gt; l'(x)).
\]</p>
<div class="admonition abstract">
<p class="admonition-title">Proof</p>
<p>For the first inequality, extend then use McMillan ineq.</p>
<p class="arithmatex">\[
\text{LHS} \le \sum_{x:p(x)\le 2^{-l'(x) - c + 1}} p(x) \le 2^{-(c-1)}.
\]</p>
<p>For the second inequality, prove</p>
<p class="arithmatex">\[
\operatorname{E}\operatorname{sgn}(l(x) - l'(x)) \le 0,
\]</p>
<p>where <span class="arithmatex">\(\operatorname{sgn}(x) \le 2^x - 1\)</span> for <span class="arithmatex">\(x\in\mathbb{Z}\)</span>.</p>
<p>Also, for any <span class="arithmatex">\(f(x) \le 2^x - 1\)</span> for <span class="arithmatex">\(x\in\mathbb{Z}\)</span>, <span class="arithmatex">\(\operatorname{E}\operatorname{sgn}f(l(x) &lt; l'(x)) \le 0\)</span>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Example: generation of discrete distributions <span class="arithmatex">\(X\)</span> from fair coins:</p>
<p>Algorithm: generate a tree, of which each leaf is assigned to a symbol <span class="arithmatex">\(y\)</span>. This tree should be binary complete. Leaf <span class="arithmatex">\(y\)</span> at depth <span class="arithmatex">\(t\)</span> has probability <span class="arithmatex">\(2^{-t}\)</span>, and the path from root to it represents its codeword. This tree has expected depth of each leaf</p>
<p class="arithmatex">\[
\operatorname{E}T = \sum_{y\in\mathcal{Y}}t(y)2^{-t(y)} = H(Y).
\]</p>
<p>Now, map <span class="arithmatex">\(Y\)</span> to <span class="arithmatex">\(X\)</span> by aggregating groups of proper <span class="arithmatex">\(y\)</span> and assign each group/single symbol to <span class="arithmatex">\(X\)</span>. This is a many-to-one mapping.</p>
<p>This is equivalent to expanding each <span class="arithmatex">\(p(x) = \sum_i2^{-k}\)</span>.</p>
<p><span class="arithmatex">\(H(X) \le H(Y)\)</span>, therefore <span class="arithmatex">\(H(X) \le \operatorname{E}T\)</span>.</p>
<p>This tree can be infinite, for example <span class="arithmatex">\(p(x=a) = 2/3\)</span> and <span class="arithmatex">\(p(x=b) = 1/3\)</span>.</p>
<p><img alt="" src="../img/infoth_5_coin_tree.svg" style="transform: scale(1.3);" /></p>
<p>When <span class="arithmatex">\(X\)</span> is dyadic, <span class="arithmatex">\(\operatorname{E}T = H(X)\)</span>.</p>
<p>Finally, the expected number of bits required <span class="arithmatex">\(\operatorname{E}T\)</span> satisfies</p>
<p class="arithmatex">\[
H(X) \le \operatorname{E}T &lt; H(X)+2.
\]</p>
</div>
<h2 id="ch-7-channel-capacity">Ch. 7: Channel capacity<a class="headerlink" href="#ch-7-channel-capacity" title="Permanent link">&para;</a></h2>
<p>A <strong>communication system</strong>:</p>
<p class="arithmatex">\[
\underset{\text{message}} W \xrightarrow{\text{encoder}} X^n \xrightarrow[p(y|x)]{\text{channel}}Y^n \xrightarrow{\text{decoder}} \underset{\text{estimate of } W} W
\]</p>
<p>A channel is said to be <strong>memoryless</strong> if its probability distribution of output depends only on the input at that time, that is, conditionally independent of previous input/output.</p>
<p>The "information" <strong>channel capacity</strong> of a discrete memoryless channel is defined as</p>
<p class="arithmatex">\[
C = \max_{p(x)} I(X;Y),
\]</p>
<p>that is the maximum number of distinguishable signals can be transmitted per use of this channel.</p>
<p>By this we have that the noiseless binary channel has capacity of 1 bit, which is achieved by <span class="arithmatex">\(p = \{0.5, 0.5\}\)</span></p>
<p>A <strong>noiseless binary channel</strong> has capacity of 1 bit, which is achieved by <span class="arithmatex">\(p = \{0.5, 0.5\}\)</span>. In this channel <span class="arithmatex">\(H(Y|X) = 0\)</span> since the knowledge of <span class="arithmatex">\(X\)</span> completely eliminates uncertainty of <span class="arithmatex">\(Y\)</span>, and it also holds for noisy channel with nonoverlapping outputs.</p>
<p><strong>Binary symmetric channel</strong>:</p>
<p><img alt="" src="../img/infoth_7_bsc.svg" style="transform: scale(1.4);" /></p>
<p class="arithmatex">\[
\begin{aligned}
C &amp;= \max_{p(x)} H(Y) - H(Y|X) \\
&amp;= 1 - H(p).
\end{aligned}
\]</p>
<p><strong>Binary erasure channel</strong>:</p>
<p><img alt="" src="../img/infoth_7_bec.svg" style="transform: scale(1.4);" /></p>
<p class="arithmatex">\[
C = \max_{p(x)} H(Y) - H(\alpha).
\]</p>
<p>The distribution of <span class="arithmatex">\(Y\)</span> is <span class="arithmatex">\(\{(1-\pi)(1-\alpha), \alpha, \pi(1-\alpha)\}\)</span>, assuming <span class="arithmatex">\(\Pr(X = 1) = \pi\)</span>, thus maximum is reached at <span class="arithmatex">\(\pi = 1/2\)</span>, <span class="arithmatex">\(C = 1-\alpha\)</span>. That is, a proportion of <span class="arithmatex">\(\alpha\)</span> bits are lost per transmit.</p>
<p>In a <strong>transition matrix</strong>, item on <span class="arithmatex">\(x\)</span>-th row and <span class="arithmatex">\(y\)</span>-th col is <span class="arithmatex">\(p(y|x)\)</span>.</p>
<p class="arithmatex">\[
p(y|x) = \begin{bmatrix}
.3 &amp; .2 &amp; .5 \\
.5 &amp; .3 &amp; .2 \\
.2 &amp; .5 &amp; .3
\end{bmatrix}
\]</p>
<p><img alt="" src="../img/infoth_7_tranmat.svg" style="transform: scale(1.4);" /></p>
<p><span class="arithmatex">\(x\)</span>-th row is the conditional distribution <span class="arithmatex">\(P(Y|X = x)\)</span>.</p>
<p class="arithmatex">\[
\begin{aligned}
p(Y = y_0)
&amp;= \sum_x p(Y = y_0, X = x) \\
&amp;= \sum_x p(Y = y_0 \mid X = x)p(X = x) \\
&amp;= \sum_{r \in \mathrm{rows}} p(r) p(Y = y_0, r) \\
&amp;= \text{weighted sum along the column of } y_0.
\end{aligned}
\]</p>
<p>In a <strong>symmetric channel</strong>, transition matrix rows are permutations of each other and cols are permutations of each other.</p>
<p>In a <strong>weakly symmetric channel</strong>, transition matrix rows are permutations of each other and all col sums are equal. It achieves uniform <span class="arithmatex">\(Y\)</span> when <span class="arithmatex">\(X\)</span> is uniformly distributed. Therefore <span class="arithmatex">\(C = \log|\mathcal{Y}| - H(r)\)</span>, <span class="arithmatex">\(r\)</span> in any row.</p>
<p>Properties of channel capacity:</p>
<ul>
<li><span class="arithmatex">\(C \ge 0\)</span>, (<span class="arithmatex">\(I(X;Y) \ge 0\)</span>).</li>
<li><span class="arithmatex">\(C \le \log|\mathcal{X}|\)</span>, <span class="arithmatex">\(C \le \log|\mathcal{Y}|\)</span>.</li>
<li><span class="arithmatex">\(I(X; Y)\)</span> is continuous and concave on <span class="arithmatex">\(p(x)\)</span>.</li>
</ul>
<p>A <strong>discrete channel</strong> is denoted by <span class="arithmatex">\((\mathcal{X}, p(y|x), \mathcal{Y})\)</span>, where <span class="arithmatex">\(\forall x \forall y, p(y|x) \in [0, 1]\)</span>, and <span class="arithmatex">\(\forall x \sum\limits_y p(y|x) = 1\)</span>.</p>
<p>A <strong>discrete memoryless channel</strong> (DMC) has its <span class="arithmatex">\(n\)</span>-th extension <span class="arithmatex">\((\mathcal{X}^n, p(y^n|x^n), \mathcal{Y}^n)\)</span>. Memorylessness is</p>
<p class="arithmatex">\[
p(y_k \mid x^k, y^{k-1}) = p(y_k \mid x_k),
\]</p>
<p>where subscript <span class="arithmatex">\(\cdot_k\)</span> denotes the <span class="arithmatex">\(k\)</span>-th element, and superscript <span class="arithmatex">\(\cdot^k\)</span> denotes the sequence of the first <span class="arithmatex">\(k\)</span> elements.</p>
<p>A DMC is said to be <strong>without feedback</strong>, when its inputs do not depend on past outputs:</p>
<p class="arithmatex">\[
p(x_k \mid x^{k-1}, y^{k-1}) = p(x_k \mid x^{k-1}).
\]</p>
<p>It satisfies</p>
<p class="arithmatex">\[
p(y^n | x^n) = \prod_{i=1}^n p(y_i | x_i).
\]</p>
<p>An <span class="arithmatex">\((M, n)\)</span> code has the codebook (the set of codewords) <span class="arithmatex">\(\{x^n(1), x^n(2), \cdots, x^n(M)\}\)</span>. Its decoding function <span class="arithmatex">\(g:\mathcal{Y}^n\rightarrow\{1, 2, \cdots, M\}\)</span> is deterministic.</p>
<p>Given message index <span class="arithmatex">\(i\)</span>, the <strong>conditional probability of error</strong> of a channel is defined by</p>
<p class="arithmatex">\[
\lambda_i = \Pr(g(Y^n) \neq i \mid X^n = x^n(i)).
\]</p>
<p>The <strong>maximal probability of error</strong> of a channel is defined by</p>
<p class="arithmatex">\[
\lambda^{(n)} = \max_{i \in \{1, 2, \cdots, M\}} \lambda_i.
\]</p>
<p>The <strong>average probability of error</strong> of a channel is defined by</p>
<p class="arithmatex">\[
P_e^{(n)} = \frac{1}{M}\sum_{i=1}^M\lambda_i.
\]</p>
<p>The rate of a <span class="arithmatex">\((M, n)\)</span> code is defined by</p>
<p class="arithmatex">\[
R = \frac{\log M}{n}
\]</p>
<p>in bits per transmission, and it is said to be <strong>achievable</strong> if there exists a sequence of <span class="arithmatex">\((\lceil2^{nR}\rceil, n)\)</span> codes (to specify among <span class="arithmatex">\(M\)</span> messages, <span class="arithmatex">\(\log M\)</span> bits are required), such that <span class="arithmatex">\(\lim\limits_{n\rightarrow \infty} \lambda^{(n)} = 0\)</span>.</p>
<p>The <strong>capacity</strong> of a channel is the supremum of all achievable rates.</p>
<p>The set of <strong>joint typical sequences</strong> is</p>
<p class="arithmatex">\[
\begin{aligned}
A_\epsilon^{(n)} = \{ &amp;(x^n, y^n) \in \mathcal{X}^n \times \mathcal{Y}^n: \\
&amp;\left|-\frac{1}{n}\log p(x^n) - H(X)\right| &lt; \epsilon, \\
&amp;\left|-\frac{1}{n}\log p(y^n) - H(Y)\right| &lt; \epsilon, \\
&amp;\left|-\frac{1}{n}\log p(x^n, y^n) - H(X, Y)\right| &lt; \epsilon
\},
\end{aligned}
\]</p>
<p>where <span class="arithmatex">\(p(x^n, y^n) = \prod\limits_{i=1}^n p(x_i, y_i)\)</span>.</p>
<p><strong>Joint AEP</strong>: if <span class="arithmatex">\((X^n, Y^n)\)</span> are i.i.d. drawn from <span class="arithmatex">\(p(x^n, y^n) = \prod\limits_{i=1}^n p(x_i, y_i)\)</span>:</p>
<p class="arithmatex">\[
\Pr\left((X^n, Y^n) \in A_\epsilon^{(n)}\right) \rightarrow 1,
\]</p>
<p class="arithmatex">\[
\left|A_\epsilon^{(n)}\right| \le 2^{n(H(X, Y) + \epsilon)},
\]</p>
<p>and if <span class="arithmatex">\((\tilde{X}^n, \tilde{Y}^n) \sim p(x^n)p(y^n)\)</span>,</p>
<p class="arithmatex">\[
\Pr\left((\tilde{X}^n, \tilde{Y}^n)\in A_\epsilon^{(n)}\right) \le 2^{-n(I(X;Y) - 3\epsilon)},
\]</p>
<p>and for sufficiently large <span class="arithmatex">\(n\)</span>,</p>
<p class="arithmatex">\[
\Pr\left((\tilde{X}^n, \tilde{Y}^n)\in A_\epsilon^{(n)}\right) \ge (1-\epsilon)2^{-n(I(X;Y) + 3\epsilon)}.
\]</p>
<p><strong>Channel coding theorem</strong>: for a DMC, and rate <span class="arithmatex">\(R &lt; C\)</span> is achievable. That is, there exists a sequence of <span class="arithmatex">\((2^{nR}, n)\)</span> codes, for <span class="arithmatex">\(\forall R &lt; C\)</span>, <span class="arithmatex">\(\lambda^{(n)} \rightarrow 0\)</span>.</p>
<p>Conversely, any sequence of <span class="arithmatex">\((2^{nR}, n)\)</span> codes with <span class="arithmatex">\(\lambda^{(n)} \rightarrow 0\)</span> must have <span class="arithmatex">\(R \le C\)</span>.</p>
<div class="admonition abstract">
<p class="admonition-title">Proof</p>
<p>Generate <span class="arithmatex">\((2^{nR}, n)\)</span> codes according to <span class="arithmatex">\(p(x^n) = \prod\limits_{i=1}^np(x_i)\)</span>, and the <span class="arithmatex">\(2^{nR}\)</span> codewords are</p>
<p class="arithmatex">\[
\mathcal{C} = \begin{bmatrix}
x_1(1), x_2(1), \cdots, x_n(1) \\
x_1(2), x_2(2), \cdots, x_n(2) \\
\vdots \\
x_1(2^{nR}), x_2(2^{nR}), \cdots, x_n(2^{nR})
\end{bmatrix},
\]</p>
<p>where <span class="arithmatex">\(w\)</span>-th row is the codeword for message <span class="arithmatex">\(w\)</span>. Therefore <span class="arithmatex">\(\Pr(\mathcal{C}) = \prod\limits_{w=1}^{2^{nR}}\prod\limits_{i=1}^{n} p(x_i(w))\)</span>,</p>
<p>In this channel, both sender and receiver knows <span class="arithmatex">\(\mathcal{C}\)</span> and <span class="arithmatex">\(p(y|x)\)</span>.</p>
<p>Choose a message <span class="arithmatex">\(W\)</span> uniformly, that is <span class="arithmatex">\(\Pr(W = w) = 2^{-nR}\)</span>, and send over the channel.</p>
<p><span class="arithmatex">\(Y^n\)</span> is received and <span class="arithmatex">\(p(y^n | x^n(w)) = \prod\limits_{i=1}^{n} p(y_i | x_i(w))\)</span></p>
<p>Receiver guesses <span class="arithmatex">\(W\)</span> by joint typical decoding: a unique <span class="arithmatex">\((X^n(\hat{W}), Y^n)\)</span> is joint typical. If not, error.</p>
<p>Let <span class="arithmatex">\(\mathcal{E}\)</span> be the event <span class="arithmatex">\(\hat{W} \neq W\)</span>, and <span class="arithmatex">\(E_i\)</span> be the event <span class="arithmatex">\((X^n(i), Y^n) \in A_\epsilon^{(n)}\)</span>. Since error spread equally on all messages, without loss of generality,</p>
<p class="arithmatex">\[
\begin{aligned}
P(\mathcal{E})
&amp;= \sum_\mathcal{C} \Pr(\mathcal{C})\overline{\lambda_w(\mathcal{C})} \\
&amp;= \frac{1}{2^{nR}}\sum_{w=1}^{2^{nR}}\sum_{\mathcal{C}}\Pr(\mathcal{C})\lambda_w(\mathcal{C}) \\
&amp;= \sum_\mathcal{C} \Pr(\mathcal{C})\lambda_1(\mathcal{C}) \\
&amp;= \Pr(\mathcal{E} \mid w=1) \\
&amp;= \Pr\left(E_i^C \cup \bigcup_{i=2}^{2^{nR}} E_i \mid w = 1\right) \\
&amp;\le \Pr(E_1^C \mid w=1) + \sum_{i=2}^{2^{nR}}\Pr(E_i \mid w=1)
\end{aligned}
\]</p>
<p>By joint AEP, for sufficiently large <span class="arithmatex">\(n\)</span>,</p>
<p class="arithmatex">\[
\Pr(E_1^C \mid w=1) = \Pr((X_1^n, Y^n) \not\in A_\epsilon^{(n)} \mid w=1) \le \epsilon,
\]</p>
<p class="arithmatex">\[
\sum_{i=2}^{2^{nR}}\Pr(E_i \mid w=1) \le \sum_{i=2}^{2^{nR}}2^{-n(I(X;Y) - 2\epsilon)}\le 2^{3n\epsilon}2^{-n(I(X;Y) - R)} \le \epsilon.
\]</p>
<p>therefore <span class="arithmatex">\(\Pr(\mathcal{E} \mid w=1) = 2\epsilon\)</span>, that is, average error probability can be arbitrarily small.</p>
<p>Choose the best codebook <span class="arithmatex">\(\mathcal{C}^*\)</span> such that <span class="arithmatex">\(\Pr(\mathcal{E} \mid \mathcal{C}^*) \le 2\epsilon\)</span>, and that implies at least half codewords <span class="arithmatex">\(i\)</span> in <span class="arithmatex">\(\mathcal{C}^*\)</span> have <span class="arithmatex">\(\lambda_i &lt; 4\epsilon\)</span>. Reindex these <span class="arithmatex">\(2^{nR-1}\)</span> codewords have rate <span class="arithmatex">\(R-\frac{1}{n}\)</span>. This new code of rate arbitrarily close to capacity, has its maximum error probability <span class="arithmatex">\(\lambda^{(n)} &lt; 4\epsilon\)</span>.</p>
</div>
<p><strong>Zero error codes</strong>: <span class="arithmatex">\(P_e^{(n)} = 0 \Rightarrow R \le C\)</span>.</p>
<div class="admonition abstract">
<p class="admonition-title">Proof</p>
<p class="arithmatex">\[
\begin{aligned}
nR &amp;\le H(W) \\
&amp;= H(W|Y^n) + I(W;Y^n) \\
\text{\footnotesize(zero error)} &amp;= I(W;Y^n) \\
{\footnotesize(W\rightarrow X^n(W) \rightarrow Y^n)} &amp;\le I(X^n; Y^n) \\
&amp;= \sum_{i=1}^n I(X_i; Y_i) \\
\text{\footnotesize(see below)} &amp;\le nC
\end{aligned}
\]</p>
</div>
<p><strong>Converse to the coding theorem</strong>: any sequence of <span class="arithmatex">\((2^{nR}, n)\)</span> codes with <span class="arithmatex">\(\lambda^{(n)} \rightarrow 0\)</span> must have <span class="arithmatex">\(R \le C\)</span>.</p>
<div class="admonition abstract">
<p class="admonition-title">Proof</p>
<p><span class="arithmatex">\(\lambda^{(n)} \rightarrow 0\)</span> implies <span class="arithmatex">\(P_e^{(n)} \rightarrow 0\)</span>. By Fano's inequality, <span class="arithmatex">\(H(W|\hat{W}) \le 1 + P_e^{(n)}nR\)</span>.</p>
<p class="arithmatex">\[
\begin{aligned}
I(X^n; Y^n) &amp;= H(Y^n) - H(Y^n | X^n) \\
&amp;= H(Y^n) - \sum_{i=1}^n H(Y_i \mid Y_1, Y_2, \cdots, Y_i-1, X^n) \\
&amp;= H(Y^n) - \sum_{i=1}^n H(Y_i \mid X_i) \\
&amp;\le \sum_{i=1}^n H(Y_i) - \sum_{i=1}^n H(Y_i \mid X_i) \\
&amp;\le \sum_{i=1}^n I(X_i ; Y_i) \\
&amp;\le nC.
\end{aligned}
\]</p>
<p>That is, capacity per transmission do not increase when channel used multiple times.</p>
<p>Hence,</p>
<p class="arithmatex">\[
\begin{aligned}
nR &amp;= H(W) \\
&amp;= H(W|\hat{W}) + I(W;\hat{W}) \\
&amp;\le 1 + P_e^{(n)}nR + I(W;\hat{W}) \\
&amp;\le 1 + P_e^{(n)}nR + I(X^n;Y^n) \\
&amp;\le 1 + P_e^{(n)}nR + nC.
\end{aligned}
\]</p>
<p>That leads to <span class="arithmatex">\(R \le P_e^{(n)}R + \dfrac{1}{n} + C\)</span> then <span class="arithmatex">\(R \le C\)</span>.</p>
<p>At <span class="arithmatex">\(R \le C\)</span>, <span class="arithmatex">\(P_e^{(n)} \rightarrow 0\)</span> exponentially; at <span class="arithmatex">\(R &gt; C\)</span>, <span class="arithmatex">\(P_e^{(n)} \rightarrow 1\)</span> exponentially.</p>
</div>
<p>The feedback capacity is the capacity when all received symbols are immediately sent back noiselessly, and used to determine what to send next:</p>
<p class="arithmatex">\[
C_{\mathrm{FB}} = C.
\]</p>
<div class="admonition abstract">
<p class="admonition-title">Proof</p>
<p>Let <span class="arithmatex">\(R\)</span> be the rate with feedback.</p>
<p class="arithmatex">\[
\begin{aligned}
nR &amp;\le 1 + P_e^{(n)}nR + I(W;\hat{W}) \\
&amp;\le 1 + P_e^{(n)}nR + I(W;Y^n).
\end{aligned}
\]</p>
<p class="arithmatex">\[
\begin{aligned}
I(W;Y^n)
&amp;= H(Y^n) - H(Y^n \mid W) \\
&amp;= H(Y^n) - \sum_{i=1}^n H(Y_i \mid Y_1, Y_2, \cdots, Y_{i-1}, W) \\
{\footnotesize (X_i \text{ determines }Y_1, \cdots, Y_{i-1}, W)}&amp;= H(Y^n) - \sum_{i=1}^n H(Y_i \mid Y_1, Y_2, \cdots, Y_{i-1}, W) \\
&amp;\le \sum_{i=1}^n H(Y_i) - \sum_{i=1}^n H(Y_i | X_i) \\
&amp;= \sum_{i=1}^nI(X_i; Y_i) \\
&amp;\le nC,
\end{aligned}
\]</p>
<p>therefore <span class="arithmatex">\(R \le C\)</span>. And because any rate achieved by non-feedback code is achievable by feedback code, <span class="arithmatex">\(C_{\mathrm{FB}} \ge C\)</span>, thus <span class="arithmatex">\(C_{\mathrm{FB}} = C\)</span>.</p>
</div>
<h2 id="ch-8-differential-entropy">Ch. 8: Differential entropy<a class="headerlink" href="#ch-8-differential-entropy" title="Permanent link">&para;</a></h2>
<p>The <strong>differential entropy</strong> of a continuous variable <span class="arithmatex">\(X\)</span> is</p>
<p class="arithmatex">\[
h(X) = -\int_S f(x)\log f(x) \mathrm{d}x.
\]</p>
<p>where <span class="arithmatex">\(f(x)\)</span> is the probability density function of <span class="arithmatex">\(X\)</span> and <span class="arithmatex">\(S\)</span> is the support set of <span class="arithmatex">\(X\)</span>, <span class="arithmatex">\(S = \{x: f(x) &gt; 0\}\)</span>.</p>
<ul>
<li>Uniform distribution: <span class="arithmatex">\(X \sim U(a, b)\)</span>, <span class="arithmatex">\(\displaystyle h(X) = -\int_a^b \frac{1}{b-a}\log\frac{1}{b-a}\mathrm{d}x = \log (b-a)\)</span>.</li>
<li>Gaussian distribution: <span class="arithmatex">\(X \sim N(0, \sigma^2)\)</span>, <span class="arithmatex">\(\displaystyle h(X) = \frac{1}{2}\ln 2\pi \mathrm{e}\sigma^2\)</span> nats, or <span class="arithmatex">\(\displaystyle \frac{1}{2}\log 2\pi \mathrm{e}\sigma^2\)</span> bits.</li>
<li>Multivariate Gaussian distribution: <span class="arithmatex">\(X \sim N_n(\mu, K)\)</span>, <span class="arithmatex">\(\displaystyle f(X) = \frac{1}{(2\pi)^{n/2} |K|^{1/2}} \exp\left(-\frac{1}{2}(x-\mu)^\top K^{-1} (x - \mu)\right)\)</span>, <span class="arithmatex">\(\displaystyle h(X) = \frac{1}{2}\log (2\pi\mathrm{e})^n|K|\)</span>.</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p class="arithmatex">\[
\operatorname{E}g(x) = \int g(x)p(x)\mathrm{d}x.
\]</p>
<p class="arithmatex">\[
\int_{-\infty}^{+\infty} N(x; 0, \sigma^2)\, x^2\mathrm{d}x = \sigma^2.
\]</p>
</div>
<p><strong>AEP</strong>: for i.i.d. <span class="arithmatex">\(X_i\)</span>,</p>
<p class="arithmatex">\[
-\frac{1}{n}\log f(X_1, X_2, \cdots, X_n) \rightarrow \operatorname{E}[-\log f(X)] = h(X) \quad \text{in probability}.
\]</p>
<p>The <strong>volume</strong> of a set <span class="arithmatex">\(A \in \mathbb{R}^n\)</span></p>
<p class="arithmatex">\[
\operatorname{Vol}(A) = \int_A \mathrm{d}x_1 \mathrm{d}x_2 \cdots \mathrm{d}x_n.
\]</p>
<p><strong>Typical set</strong>:</p>
<p class="arithmatex">\[
A_\epsilon^{(n)} = \left\{(x_1, x_2, \cdots, x_n) \in S^n: \left|-\frac{1}{n}\log f(x_1, x_2, \cdots, x_n) - h(X)\right| \le \epsilon \right\},
\]</p>
<p>where <span class="arithmatex">\(f(x_1, x_2, \cdots, x_n) = \prod_i f(x_i)\)</span>.</p>
<p>For <span class="arithmatex">\(n\)</span> sufficiently large,</p>
<p class="arithmatex">\[
\Pr\left(A_\epsilon^{(n)}\right) &gt; 1 - \epsilon.
\]</p>
<p>For all <span class="arithmatex">\(n\)</span>,</p>
<p class="arithmatex">\[
\operatorname{Vol}\left(A_\epsilon^{(n)}\right) \le 2^{n(h(X)+\epsilon)}.
\]</p>
<p>For <span class="arithmatex">\(n\)</span> sufficiently large,</p>
<p class="arithmatex">\[
\operatorname{Vol}\left(A_\epsilon^{(n)}\right) \ge (1 - \epsilon)2^{n(h(X)-\epsilon)}.
\]</p>
<p><span class="arithmatex">\(A_\epsilon^{(n)}\)</span> is the smallest volume set with probability <span class="arithmatex">\(\ge 1 - \epsilon\)</span>, to the first order in exponential.</p>
<p><strong>Relation to discrete entropy</strong>: quantize <span class="arithmatex">\(X\)</span> into equal bins with length <span class="arithmatex">\(\varDelta\)</span>. According to mean value theorem, <span class="arithmatex">\(p_i = \int_{i\varDelta}^{(i+1)\varDelta}f(x)\mathrm{d}x = f(x_i)\varDelta\)</span>. Entropy of the quantized version</p>
<p class="arithmatex">\[
h(X^\varDelta) = \sum_i \varDelta f(x_i)\log\frac{1}{f(x_i)} - \log\varDelta.
\]</p>
<p>Therefore, <span class="arithmatex">\(H(X^\varDelta) + \log\varDelta \rightarrow h(X)\)</span> as <span class="arithmatex">\(\varDelta \rightarrow 0\)</span>. Quantize <span class="arithmatex">\(X\)</span> into 2^n equal sections, the entropy is approximately <span class="arithmatex">\(h(X) + n\)</span>.</p>
<div class="admonition note">
<p class="admonition-title">Example</p>
<p><span class="arithmatex">\(X \sim U(0, 1)\)</span>, <span class="arithmatex">\(h(X) = 0\)</span>. If quantization bin length <span class="arithmatex">\(\varDelta = 2^n\)</span>, <span class="arithmatex">\(H(X^\varDelta) = n\)</span>. It requires <span class="arithmatex">\(n\)</span> bits to describe <span class="arithmatex">\(X\)</span> to <span class="arithmatex">\(n\)</span> bits (<span class="arithmatex">\(2^{-n}\)</span>) accuracy.</p>
<p>In general, <span class="arithmatex">\(h(X) + n\)</span> is the number of bits required on average to describe an continuous random variable to <span class="arithmatex">\(n\)</span> bits accuracy.</p>
</div>
<p><strong>Joint differential entropy</strong>:</p>
<p class="arithmatex">\[
\begin{aligned}
h(X_1, X_2, \cdots, X_n)
&amp;= -\int f(x_1, x_2, \cdots, x_n)\log f(x_1, x_2, \cdots, x_n)\mathrm{d}x_1\mathrm{d}x_2\cdots\mathrm{d}x_n \\
&amp;= -\operatorname{E} \log f(x_1, x_2, \cdots, x_n).
\end{aligned}
\]</p>
<p><span class="arithmatex">\(h(X_1, X_2, \cdots, X_n) \le \sum h(X_i)\)</span>, with equality iff all <span class="arithmatex">\(X_i\)</span> are independent.</p>
<p><strong>Conditional differential entropy</strong>:</p>
<p class="arithmatex">\[
\begin{aligned}
h(X|Y) &amp;= - \int f(x, y)\log f(x | y)\mathrm{d}x\mathrm{d}y \\
&amp;= -\operatornamewithlimits{E}_{f(x,y)}\log f(x|y) \\
&amp;= h(X, Y) - h(Y).
\end{aligned}
\]</p>
<p>The last equality holds if both items are finite.</p>
<p><span class="arithmatex">\(h(X | Y) \le h(X)\)</span>, with equality iff <span class="arithmatex">\(X\)</span> and <span class="arithmatex">\(Y\)</span> are independent.</p>
<p><strong>Relative entropy</strong>:</p>
<p class="arithmatex">\[
D(f \| g) = \int_{S_f} f \log \frac{f}{g},
\]</p>
<p><span class="arithmatex">\(S_f \subseteq S_g\)</span> if the relative entropy is finite.</p>
<p><span class="arithmatex">\(D(f\|g) \ge 0\)</span>, with equality iff <span class="arithmatex">\(f = g\)</span> almost everywhere.</p>
<p><strong>Mutual information</strong>:</p>
<p class="arithmatex">\[
\begin{aligned}
I(X;Y) &amp;= D(f(x, y) \| f(x)f(y)) \\
&amp;= \int f(x, y)\log\frac{f(x, y)}{f(x)f(y)} \mathrm{d}x\mathrm{d}y.
\end{aligned}
\]</p>
<p>If there are two finite partitions <span class="arithmatex">\(P\)</span> and <span class="arithmatex">\(Q\)</span> that quantize <span class="arithmatex">\(X\)</span> into discrete random variables <span class="arithmatex">\([X]_P\)</span> and <span class="arithmatex">\([X]_Q\)</span> respectively,</p>
<p class="arithmatex">\[
I(X;Y) = \sup_{P, Q} I([X]_P, [X]_Q).
\]</p>
<p>This applies to even distributions with singular density parts.</p>
<p><span class="arithmatex">\(I(X; Y) \ge 0\)</span>, with equality iff <span class="arithmatex">\(X\)</span> and <span class="arithmatex">\(Y\)</span> are independent.</p>
<div class="admonition note">
<p class="admonition-title">Example: correlated Gaussian random variables</p>
<p class="arithmatex">\[
(X, Y) \sim N(\boldsymbol{0}, K), \quad K =
\begin{bmatrix}
\sigma^2 &amp; \rho\sigma^2 \\
\rho\sigma^2 &amp; \sigma^2
\end{bmatrix}.
\]</p>
<p class="arithmatex">\[
h(X, Y) = \frac{1}{2}\log(2\pi\mathrm{e})^2(1-\rho^2)\sigma^4.
\]</p>
<p class="arithmatex">\[
I(X; Y) = h(X) + h(Y) - h(X, Y) = -\frac{1}{2}\log(1 - \rho^2).
\]</p>
<p>When <span class="arithmatex">\(\rho = 0\)</span>, <span class="arithmatex">\(X\)</span> and <span class="arithmatex">\(Y\)</span> are independent, <span class="arithmatex">\(I = 0\)</span>.</p>
<p>When <span class="arithmatex">\(\rho = \pm 1\)</span>, <span class="arithmatex">\(X\)</span> and <span class="arithmatex">\(Y\)</span> are perfectly correlated, <span class="arithmatex">\(I = +\infty\)</span>. While in discrete cases, <span class="arithmatex">\(I(X; Y) = H(X) = H(Y)\)</span>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Joint differential entropy of fully dependent r.v.s: <span class="arithmatex">\(X \sim U(0, 1), Y = X, H(X, Y) = \ ?\)</span></p>
<p>First we derive the joint probability density of <span class="arithmatex">\(X\)</span> and <span class="arithmatex">\(Y\)</span> step by step. The probability density of <span class="arithmatex">\(X\)</span> is</p>
<p class="arithmatex">\[
p(x) = \begin{cases}
1, &amp; x \in (0, 1), \\
0, &amp; \text{otherwise}.
\end{cases}
\]</p>
<p>And the conditional probability density <span class="arithmatex">\(p(y|x) = \delta(y-x)\)</span>, where <span class="arithmatex">\(\delta\)</span> is the Dirac delta function. Therefore,</p>
<p class="arithmatex">\[
p(x, y) = p(y|x)p(x) = \delta(y-x)p(x).
\]</p>
<p>Check unit measure property: <span class="arithmatex">\(\int_0^1 \delta(y-x)\mathrm{d}x = p(y) = p(x)\)</span>, or, <span class="arithmatex">\(\forall y\in (0, 1), 0 \in \{y - x: x\in (0, 1)\}\)</span>, thus <span class="arithmatex">\(\int_0^1 \delta(y-x)\mathrm{d}x = 1\)</span>. Then <span class="arithmatex">\(\int_\mathbb{R} p(x, y) \mathrm{d}x\mathrm{d}y = \int_0^1\int_0^1 \delta(y-x)p(x) \mathrm{d}x\mathrm{d}y = 1\)</span>.</p>
<p class="arithmatex">\[
\begin{aligned}
h(X, Y) &amp;= -\int_0^1\int_0^1 \delta(y-x)p(x)\log(\delta(y-x)p(x))\mathrm{d}x\mathrm{d}y\\
&amp;= -\int_0^1\int_0^1 \delta(y-x)\log\delta(y-x)\mathrm{d}x\mathrm{d}y.\\
\end{aligned}
\]</p>
<p>Let <span class="arithmatex">\(u(y) = \int_0^1 \delta(y-x)\log\delta(y-x)\mathrm{d}x\)</span> for <span class="arithmatex">\(x \in (0, 1)\)</span>, with domain <span class="arithmatex">\(y \in (0, 1)\)</span>.</p>
<p>Dirac delta function can be represented by <span class="arithmatex">\(\displaystyle\delta(x) = \lim_{a\rightarrow 0} \delta_a(x)\)</span>, where</p>
<p class="arithmatex">\[
\delta_a(x) = \begin{cases}
\dfrac{1}{2a}, &amp; x\in (-a, a) \\
0, &amp;\text{otherwise}.
\end{cases}
\]</p>
<p>Thus, for any <span class="arithmatex">\(x_0 \in (0, 1)\)</span>,</p>
<p class="arithmatex">\[
\begin{aligned}
u(y) &amp;= \lim_{a\rightarrow 0} \int_0^1 \delta_a(y-x_0)\log\delta_a(y-x_0)\mathrm{d}y \\
&amp;= \lim_{a\rightarrow 0} \int_{x_0 - a}^{x_0 + a} \frac{1}{2a}\log\frac{1}{2a}\mathrm{d}y \\
&amp;= \lim_{a\rightarrow 0} \log\frac{1}{2a} \\
&amp;= +\infty.
\end{aligned}
\]</p>
<p>The joint differential entropy <span class="arithmatex">\(h(X, Y) = -\int_0^1 u(y) \mathrm{d}y = -\infty\)</span>, and mutual information <span class="arithmatex">\(I(X, Y) = h(X) + h(Y) - h(X, Y) = +\infty\)</span>. In general, if probability distribution occupies manifold of less dimension than the space, its differential entropy is <span class="arithmatex">\(-\infty\)</span>.</p>
</div>
<p><strong>Chain rule for differential entropy</strong>: same as that in the discrete scenarios,</p>
<p class="arithmatex">\[
h(X_1, X_2, \cdots, X_n) = \sum_{i=1}^n h(X_i \mid X_1, X_2, \cdots, X_{i-1}).
\]</p>
<p><strong>Hadamard's inequality</strong>:</p>
<p class="arithmatex">\[
|K| \le \prod K_{ii}.
\]</p>
<p><strong>Entropy of linear function of <span class="arithmatex">\(X\)</span></strong>:</p>
<p class="arithmatex">\[
h(X + c) = h(X).
\]</p>
<p class="arithmatex">\[
h(aX) = h(X) + \log |a|.
\]</p>
<p class="arithmatex">\[
h(AX) = h(X) + \log \left|\det A \right|.
\]</p>
<p>For <span class="arithmatex">\(X\in \mathbb{R}^n\)</span> that have mean <span class="arithmatex">\(\boldsymbol{0}_n\)</span> and covariance <span class="arithmatex">\(\operatorname{E}XX^\top = K\)</span>,</p>
<p class="arithmatex">\[
h(X) \le \frac{1}{2}\log(2\pi\mathrm{e})^n |K|,
\]</p>
<p>with equality iff <span class="arithmatex">\(X \sim N(\boldsymbol{0}, K)\)</span>. That is, <strong>Gaussian maximizes entropy</strong> over all distributions with the same variance.</p>
<p>Let <span class="arithmatex">\(\hat{X}\)</span> be any estimator of <span class="arithmatex">\(X\in\mathbb{R}\)</span>, (<span class="arithmatex">\(X\)</span> is estimated to be some known value <span class="arithmatex">\(\hat{X}\)</span>),</p>
<p class="arithmatex">\[
\operatorname{E}(X - \hat{X})^2 \ge \frac{1}{2\pi\mathrm{e}} \mathrm{e}^{2h(X)},
\]</p>
<p>with equality iff <span class="arithmatex">\(X \sim N(\hat{X}, \sigma^2)\)</span>.</p>
<div class="admonition abstract">
<p class="admonition-title">Proof</p>
<p class="arithmatex">\[
\begin{aligned}
\operatorname{E}(X - \hat{X})^2 &amp;\ge \min_{\hat{X}} \operatorname{E}(X - \hat{X})^2, \\
&amp;= \operatorname{E}(X - \operatorname{E}(X))^2 \\
&amp;= \operatorname{Var}(X) \\
&amp;\ge \frac{1}{2\pi\mathrm{e}} \mathrm{e}^{2h(X)}.
\end{aligned}
\]</p>
</div>
<p>Generally, given side information <span class="arithmatex">\(Y\)</span>, estimator <span class="arithmatex">\(\hat{X}(Y)\)</span> satisfies</p>
<p class="arithmatex">\[
\operatorname{E}(X - \hat{X}(Y))^2 \ge \frac{1}{2\pi\mathrm{e}}\mathrm{e}^{2h(X|Y)}.
\]</p>
<h2 id="ch-9-gaussian-channel">Ch. 9: Gaussian channel<a class="headerlink" href="#ch-9-gaussian-channel" title="Permanent link">&para;</a></h2>
<p>A <strong>Gaussian channel</strong> is a time-discrete channel with output <span class="arithmatex">\(Y_i\)</span>, input <span class="arithmatex">\(X_i\)</span> and Gaussian noise <span class="arithmatex">\(Z_i\)</span> at time <span class="arithmatex">\(i\)</span></p>
<p class="arithmatex">\[
Y_i = X_i + Z_i,\quad Z \sim N(0, N).
\]</p>
<p>With power constraint <span class="arithmatex">\(P\)</span>, the information capacity is</p>
<p class="arithmatex">\[
C = \max_{f(x):\ \operatorname{E}X^2 \le P} I(X; Y) = \frac{1}{2}\log\left(1 + \frac{P}{N}\right).
\]</p>
<div class="admonition abstract">
<p class="admonition-title">Proof</p>
<p class="arithmatex">\[
\begin{aligned}
I(X;Y) &amp;= h(Y) - h(X + Z \mid X) \\
&amp;= h(Y) - h(Z \mid X) \\
\text{\footnotesize(independence)} &amp;= h(Y) - h(Z).
\end{aligned}
\]</p>
<p class="arithmatex">\[
\operatorname{E}Y^2 = \operatorname{E}X^2 + 2\operatorname{E}X\operatorname{E}Z + \operatorname{E}Z^2 \le P + N.
\]</p>
<p>Note that <span class="arithmatex">\(\dfrac{Z}{\sqrt{n}} \sim \chi^2(1)\)</span> therefore <span class="arithmatex">\(\operatorname{E}\dfrac{Z}{\sqrt{n}} = 1\)</span>.</p>
<p class="arithmatex">\[
\begin{aligned}
I(X;Y)
&amp;\le \frac{1}{2}\log 2\pi\mathrm{e}(P+N) - \frac{1}{2}\log 2\pi\mathrm{e}N \\
&amp;= \frac{1}{2}\log \left(1 + \frac{P}{N}\right)
\end{aligned}
\]</p>
</div>
<p>An <strong><span class="arithmatex">\((M, n)\)</span> code</strong> for a Gaussian channel consists of</p>
<ul>
<li>index set <span class="arithmatex">\(\{1, 2, \cdots, M\}\)</span>,</li>
<li>encoding function <span class="arithmatex">\(x: \{1, 2, \cdots M\}\mapsto \mathcal{X}^n\)</span>, yielding codewords <span class="arithmatex">\(x^n(\cdot)\)</span>, and constraint on power for each message <span class="arithmatex">\(\sum_{i=1}^n x_i^2(w) \le nP\)</span> for <span class="arithmatex">\(w = 1, 2, \cdots M\)</span>,</li>
<li>decoding function <span class="arithmatex">\(g: \mathcal{Y} \mapsto \{1, 2, \cdots M\}\)</span>.</li>
</ul>
<p><strong>The capacity of Gaussian channel</strong> with power constraint <span class="arithmatex">\(P\)</span> and noise variance <span class="arithmatex">\(N\)</span> achievable is</p>
<p class="arithmatex">\[
C = \frac{1}{2}\log\left(1 + \frac{P}{N}\right).
\]</p>
<div class="admonition abstract">
<p class="admonition-title">Proof</p>
<p>Generate a codebook <span class="arithmatex">\(X_i(w)\)</span> i.i.d. <span class="arithmatex">\(\sim N(0, P-\epsilon)\)</span>, <span class="arithmatex">\(i = 1, 2, \cdots n\)</span>, <span class="arithmatex">\(w = 1, 2, \cdots, 2^{nR}\)</span> and encode into codewords <span class="arithmatex">\(X^n(w)\)</span>. These codewords are decoded to <span class="arithmatex">\(\hat{W} = w\)</span>, or error. Without loss of generality, <span class="arithmatex">\(w = 1\)</span> was sent, thus <span class="arithmatex">\(Y^n = X^n(1) + Z^n\)</span>.</p>
<p>Define <span class="arithmatex">\(E_0 = \left\{\frac{1}{n}\sum X_i^2(1) &gt; P\right\}\)</span>, <span class="arithmatex">\(E_i = (X^n(i), Y^n) \in A_\epsilon^{(n)}\)</span>. Error occurs if <span class="arithmatex">\(E_0\)</span> or <span class="arithmatex">\(E_1^C\)</span> or <span class="arithmatex">\(E_2 \cup E_3 \cup \cdots \cup E_{2^{nR}}\)</span>, denoted by <span class="arithmatex">\(\mathcal{E}\)</span>.</p>
<p class="arithmatex">\[
\Pr(\mathcal{E} \mid W = 1) \le P(E_0) + P(E_1^C) + \sum_{i=2}^{2^{nR}} P(E_i).
\]</p>
<p>Law of large numbers indicates that <span class="arithmatex">\(P(E_0) \rightarrow 0\)</span> as <span class="arithmatex">\(n \rightarrow 0\)</span>. And <span class="arithmatex">\(P(E_1^C) \le \epsilon\)</span> for <span class="arithmatex">\(n\)</span> sufficiently large by joint AEP.</p>
<p class="arithmatex">\[
P_e^{(n)} = \Pr(\mathcal{E}\mid W = 1) \le \epsilon + \epsilon + \sum_{i=2}^{2^{nR}} 2^{-n(I(X;Y)-3\epsilon)} \le 3\epsilon
\]</p>
<p>for <span class="arithmatex">\(n\)</span> sufficiently large. Thus, <span class="arithmatex">\(R &lt; I(X;Y) -3\epsilon\)</span>.</p>
</div>
<p><strong>Converse to the coding theorem for Gaussian channels</strong>: if <span class="arithmatex">\(P_e^{(n)} \rightarrow 0\)</span> for a sequence of <span class="arithmatex">\((2^{nR}, n)\)</span> codes, then</p>
<p class="arithmatex">\[
R\le C = \frac{1}{2}\log \left(1 + \frac{P}{N}\right)
\]</p>
<div class="admonition abstract">
<p class="admonition-title">Proof</p>
<p>By Fano's inequality, <span class="arithmatex">\(H(W|\hat{W}) \le 1 + nRP_e^{(n)}\)</span>.</p>
<p class="arithmatex">\[
\begin{aligned}
nR &amp;= H(W) \\
&amp;= I(W; \hat{W}) + H(W | \hat{W}) \\
&amp;\le I(X^n; Y^n) + 1 + nRP_e^{(n)} \\
&amp;= h(Y^n) - h(Y^n | X^n) + 1 + nRP_e^{(n)} \\
&amp;\le \sum h(Y_i) - \sum h(Z_i) + 1 + nRP_e^{(n)} \\
&amp;= \sum I(X_i; Y_i) + 1 + nRP_e^{(n)}.
\end{aligned}
\]</p>
<p>Let <span class="arithmatex">\(P_i\)</span> be the average power of <span class="arithmatex">\(i\)</span>th column in the codebook, thus <span class="arithmatex">\(h(Y_i) \le \frac{1}{2}\log 2\pi\mathrm{e}(P_i+N)\)</span>.</p>
<p class="arithmatex">\[
\begin{aligned}
R &amp;\le \frac{1}{n} \sum \frac{1}{2} \log \left(1 + \frac{P_{i}}{N}\right) + 1 + nRP_{e}^{(n)} \\
\text{\footnotesize(Jensen)} &amp;\le \frac{1}{2} \log \left(1 + \frac{1}{n} \sum \frac{P_{i}}{N}\right) + 1 + nRP_{e}^{(n)}\\
{\footnotesize (\bar{P}_{i} \le P)}  &amp;\le \frac{1}{2}\log \left(1 + \frac{P}{N}\right) + 1 + nRP_{e}^{(n)}.
\end{aligned}
\]</p>
<p>Therefore <span class="arithmatex">\(R \le \frac{1}{2} \log \left(1 + \frac{P}{N}\right) + \frac{1}{n} + RP_{e}^{(n)}\)</span>, <span class="arithmatex">\(R \le C\)</span>.</p>
</div>
<p><strong>Parallel Gaussian channels</strong>: distribute the total power among the <span class="arithmatex">\(k\)</span> independent channels to maximize capacity. For channel <span class="arithmatex">\(j = 1, 2, \cdots, k\)</span>,</p>
<p class="arithmatex">\[
Y_j = X_j + Z_j, \quad Z_j \sim N(0, N_j) \text{ independently}.
\]</p>
<p>The total power constraint is <span class="arithmatex">\(\operatorname{E}\sum X_j^2 \le P\)</span>. The optimal distribution of powers is identical to water-filling powers into channels.</p>
<p><img alt="" src="../img/infoth_9_waterfilling.svg" /></p>
<div class="admonition abstract">
<p class="admonition-title">Proof</p>
<p>The information capacity</p>
<p class="arithmatex">\[
C = \max_{f(x_1, \cdots, x_k)} I(X_1, X_2, \cdots, X_k; Y_1, Y_2, \cdots Y_k).
\]</p>
<p class="arithmatex">\[
\begin{aligned}
I\left(X_{1}, \ldots, X_{k}; Y_{1}, \ldots, Y_{k}\right)
&amp;= h\left(Y_{1}, \ldots, Y_{k}\right)-h\left(Z_{1}, \ldots, Z_{k}\right) \\
&amp;= h\left(Y_{1}, \ldots, Y_{k}\right)-\sum h\left(Z_{i}\right) \\
&amp;\le \sum h\left(Y_{i}\right)-h\left(Z_{i}\right) \\
&amp;\le \sum \frac{1}{2} \log \left(1+\frac{P_{i}}{N_{i}}\right),
\end{aligned}
\]</p>
<p>where <span class="arithmatex">\(P_i = \operatorname{E}X_i^2\)</span>, <span class="arithmatex">\(\sum P_i = P\)</span>, with equality when <span class="arithmatex">\((X_1, X_2, \cdots, X_k) \sim N\left(\boldsymbol{0}, \left[\begin{smallmatrix}
P_1 &amp; &amp; &amp; \\ &amp; P_2 &amp; &amp; \\ &amp; &amp; \ddots &amp; \\ &amp; &amp; &amp; P_k
\end{smallmatrix}\right]\right)\)</span>.</p>
<p>With constraint on <span class="arithmatex">\(P_i\)</span> that <span class="arithmatex">\(\sum P_i = P\)</span>, allot those powers to maximize capacity, using Lagrange multipliers</p>
<p class="arithmatex">\[
J(P_1, P_2, \cdots, P_k) = \sum_{i=1}^k\frac{1}{2}\log\left(1+\frac{P_{i}}{N_{i}}\right) + \lambda \sum_{i=1}^k P_i,
\]</p>
<p>and with the additional nonnegative constraint <span class="arithmatex">\(P_i \ge 0\)</span>, results in</p>
<p class="arithmatex">\[
P_i = \max(0, \nu - N_i),
\]</p>
<p>where <span class="arithmatex">\(\nu\)</span> is chosen so that <span class="arithmatex">\(\sum P_i = P\)</span>.</p>
</div>
<h2 id="ch-10-rate-distortion-theory">Ch. 10: Rate distortion theory<a class="headerlink" href="#ch-10-rate-distortion-theory" title="Permanent link">&para;</a></h2>
<p>A source produces a sequence <span class="arithmatex">\(X_1, X_2, \cdots, X_n\)</span> i.i.d. <span class="arithmatex">\(\sim p(x)\)</span>, <span class="arithmatex">\(x \in \mathcal{X}\)</span>. The encoder describes each <span class="arithmatex">\(X^n\)</span> by an index <span class="arithmatex">\(f_n(X^n) \in \{1, 2, \cdots, 2^{nR}\}\)</span>. The decoder estimates <span class="arithmatex">\(X^n\)</span> by <span class="arithmatex">\(\hat{X}^n \in \hat{\mathcal{X}}\)</span>.</p>
<p>A <strong>distortion function/measure</strong> is a measure of the cost (nonnegative) representing symbol <span class="arithmatex">\(x\)</span> by symbol <span class="arithmatex">\(\hat{x}\)</span>:</p>
<p class="arithmatex">\[
d: \mathcal{X} \times \hat{\mathcal{X}} \mapsto \mathbb{R}^+.
\]</p>
<p>It is said to be bounded if <span class="arithmatex">\(\displaystyle d_{\max} = \max_{\mathcal{X} \times \hat{\mathcal{X}}} d &lt; \infty\)</span>.</p>
<p><strong>Hamming distortion</strong>:</p>
<p class="arithmatex">\[
d(x, \hat{x}) = I(x \neq \hat{x}),
\]</p>
<p>thus <span class="arithmatex">\(\operatorname{E}d = \Pr\{X \neq \hat{X}\}\)</span>.</p>
<p><strong>Squared-error distortion</strong>:</p>
<p class="arithmatex">\[
d(x, \hat{x}) = (x - \hat{x})^2.
\]</p>
<p><strong>Distortion between sequences</strong>:</p>
<p class="arithmatex">\[
d(x^n, \hat{x}^n) = \frac{1}{n}\sum d(x_i, \hat{x}_i).
\]</p>
<p>A <strong><span class="arithmatex">\((2^{nR}, n)\)</span>-rate distortion code</strong> consists of</p>
<ul>
<li>encoding function <span class="arithmatex">\(f_n: \mathcal{X}^n \mapsto \{1, 2, \cdots, 2^{nR}\}\)</span>,</li>
<li>decoding function <span class="arithmatex">\(g_n: \{1, 2, \cdots, 2^{nR}\} \mapsto \hat{\mathcal{X}}^n\)</span>.</li>
</ul>
<p>The distortion is</p>
<p class="arithmatex">\[
D = \operatorname{E}d(X^n, g_nf_n(X^n)) = \sum_{x^n}p(x^n)d(X^n, g_nf_n(X^n)).
\]</p>
<p><span class="arithmatex">\(g_n(1) = \hat{X}_n(1), g_n(2) = \hat{X}_n(2), \cdots\)</span> constitute the codebook, and <span class="arithmatex">\(f_n^{-1}(1), f_n^{-1}(2)\)</span> constitute the assignment region.</p>
<p>A rate distortion pair is <strong>achievable</strong> if there exists a sequence of <span class="arithmatex">\((2^{nR}, n)\)</span>-rate distortion codes <span class="arithmatex">\((f_n, g_n)\)</span>, with <span class="arithmatex">\(\displaystyle \lim_{n\rightarrow \infty}\operatorname{E}d \le D\)</span>.</p>
<p>The <strong>rate distortion region</strong> is the closure of the achievable <span class="arithmatex">\((R, D)\)</span>.</p>
<p>The <strong>rate distortion function</strong> <span class="arithmatex">\(R(D)\)</span> is the infimum of <span class="arithmatex">\(R\)</span> such that <span class="arithmatex">\((R, D)\)</span> is in the rate distortion region given <span class="arithmatex">\(D\)</span>.</p>
<p>The <strong>distortion rate function</strong> <span class="arithmatex">\(D(R)\)</span> is the infimum of <span class="arithmatex">\(D\)</span> such that <span class="arithmatex">\((R, D)\)</span> is in the rate distortion region given <span class="arithmatex">\(R\)</span>.</p>
<p>The information rate distortion function <span class="arithmatex">\(R^{(I)}(D)\)</span> is</p>
<p class="arithmatex">\[
R^{(I)}(D) = \min_{p(\hat{x}|x):\ \sum\limits_{(x, \hat{x})}p(x)p(\hat{x}|x)d(x,\hat{x}) \le D } I(X; \hat{X}),
\]</p>
<p>that is the minimum <span class="arithmatex">\(I\)</span> over all <span class="arithmatex">\(p(\hat{x}|x)\)</span>, for which the distortion expected on the joint distribution satisfies constraint.</p>
<p>For an i.i.d. source <span class="arithmatex">\(X\)</span> and a bounded distortion,</p>
<p class="arithmatex">\[
R(D) = R^{(I)}(D),
\]</p>
<p>and it is the minimum achievable rate at distortion <span class="arithmatex">\(D\)</span>.</p>
<div class="admonition note">
<p class="admonition-title">Example</p>
<p>The rate distortion function for <span class="arithmatex">\(\operatorname{Bernoulli}(p)\)</span> with Hamming distortion</p>
<p class="arithmatex">\[
R(D) = \begin{cases}
H(p) - H(D), &amp; 0 \le D \le \min(p, 1-p), \\
0, &amp; D \ge \min(p, 1-p).
\end{cases}
\]</p>
<p>Proof: Assume <span class="arithmatex">\(p &lt; \frac{1}{2}\)</span>,</p>
<p class="arithmatex">\[
\begin{aligned}
I(X; \hat{X})
&amp;= H(X) - H(X\mid\hat{X}) \\
&amp;= H(p) - H(X\oplus\hat{X}\mid\hat{X}) \\
&amp;\ge H(p) - H(X\oplus\hat{X}) \\
&amp;\ge H(p) - H(D),
\end{aligned}
\]</p>
<p>since <span class="arithmatex">\(\Pr(X\neq\hat{X}) = \Pr(X\oplus\hat{X} = 1) \le D\)</span>, and <span class="arithmatex">\(H(D)\)</span> decreases on <span class="arithmatex">\(D \le \frac{1}{2}\)</span>.</p>
<p>Next we prove <span class="arithmatex">\(H(p) - H(D)\)</span> is achievable. For the decoding shown in the graph, <span class="arithmatex">\(r(1-D) + (1-r)D = p\)</span>, therefore <span class="arithmatex">\(r = \frac{p-D}{1-2D}\)</span>.</p>
<p><img alt="" src="../img/infoth_10_rdber.svg" style="transform: scale(1.4);" /></p>
<ul>
<li>If <span class="arithmatex">\(D \le p \le \frac{1}{2}\)</span>, <span class="arithmatex">\(\Pr(\hat{X} = 1) \ge 0\)</span>, <span class="arithmatex">\(\Pr(\hat{X} = 0) \ge 0\)</span>, and <span class="arithmatex">\(I(X; \hat{X}) = H(p) - H(D)\)</span> is reached.</li>
<li>If <span class="arithmatex">\(D \ge p\)</span>, <span class="arithmatex">\(R(D) = 0\)</span> when <span class="arithmatex">\(\Pr(\hat{X} = 0) = 1\)</span>.</li>
<li>If <span class="arithmatex">\(D \ge 1 - p\)</span>, <span class="arithmatex">\(R(D) = 0\)</span> when <span class="arithmatex">\(\Pr(\hat{X} = 1)\)</span>.</li>
</ul>
</div>
<div class="admonition note">
<p class="admonition-title">Example</p>
<p>The rate distortion function for a <span class="arithmatex">\(N(0, \sigma^2)\)</span> source with squared error distortion is</p>
<p class="arithmatex">\[
R(D) = \begin{cases}
\displaystyle \frac{1}{2}\log\frac{\sigma^2}{D}, &amp; 0 \le D \le \sigma^2, \\
0, &amp; D &gt; \sigma^2.
\end{cases}
\]</p>
<p>Proof:</p>
<p class="arithmatex">\[
R(D) = \min_{f(\hat{x}|x):\ \operatorname{E}(\hat{x}-x)^2\le D} I(X; Y)
\]</p>
<p class="arithmatex">\[
\begin{aligned}
I(X; \hat{X})
&amp;= h(X) - h(X\mid \hat{X}) \\
&amp;= h(X) - h(X - \hat{X} \mid \hat{X}) \\
&amp;\ge h(X) - h(X - \hat{X}) \\
{\footnotesize (\operatorname{E}(X - \hat{X})^2 \le D)}&amp;\ge h(X) - h(N(0, \operatorname{E}(X - \hat{X})^2)) \\
&amp;= h(X) - \frac{1}{2}\log (2\pi\mathrm{e})\operatorname{E}(X - \hat{X})^2 \\
&amp;\ge \frac{1}{2}\log 2\pi\mathrm{e}\sigma^2 - \frac{1}{2}\log 2\pi\mathrm{e}D \\
&amp;= \frac{1}{2}\log\frac{\sigma^2}{D}.
\end{aligned}
\]</p>
<p>To achieve this lower bound, construct test channel <span class="arithmatex">\(f(\hat{x}|x)\)</span>,</p>
<p class="arithmatex">\[
X = \hat{X} + Z,
\]</p>
<p>where <span class="arithmatex">\(\hat{X}\sim N(0, \sigma^2 - D)\)</span> and <span class="arithmatex">\(Z \sim N(0, D)\)</span> independently. Note that <span class="arithmatex">\(N(\mu_1, \sigma_1^2) + N(\mu_2, \sigma_2^2) = N(\mu_1 + \mu_2, \sigma_1^2 + \sigma_2^2)\)</span>. In this scenario, <span class="arithmatex">\(I(X; \hat{X}) = \frac{1}{2}\log\frac{\sigma^2}{D}\)</span>.</p>
<p>If <span class="arithmatex">\(D &gt; \sigma^2\)</span>, choose <span class="arithmatex">\(\Pr(\hat{X} = 0) = 1\)</span>, thus <span class="arithmatex">\(R(D) = 0\)</span>.</p>
<p>The distortion rate function <span class="arithmatex">\(D(R) = \sigma^2 2^{-2R}\)</span>.</p>
</div>
<p><strong>Parallel Gaussian sources</strong> of independent Gaussian random variables</p>
<p class="arithmatex">\[
X_i \sim N(0, \sigma_i^2), \quad i = 1, 2, \cdots, m.
\]</p>
<p class="arithmatex">\[
\begin{aligned}
R(D)
&amp;= \min_{f(\hat{x}^m | x^m):\ \operatorname{E}d(\hat{X}^m | X^m) \le D} I(X^m; \hat{X}^m) \\
&amp;= \sum_{i=1}^m \frac{1}{2}\log\frac{\sigma_i^2}{D_i},
\end{aligned}
\]</p>
<p>where <span class="arithmatex">\(D_i = \min(\lambda, \sigma_i^2)\)</span>, and <span class="arithmatex">\(\lambda\)</span> is chosen so that <span class="arithmatex">\(\sum D_i = D\)</span>. This is similar to a reverse water-filling, choosing a constant <span class="arithmatex">\(\lambda\)</span> and only describe random variables with variance less than <span class="arithmatex">\(\lambda\)</span>.</p>
<p><img alt="" src="../img/infoth_10_revwaterfilling.svg" /></p>
<p><span class="arithmatex">\(R(D)\)</span> is a convex function of <span class="arithmatex">\(D\)</span> and it is nonincreasing.</p>
<p>For any source drawn i.i.d. from <span class="arithmatex">\(p(x)\)</span> with distortion <span class="arithmatex">\(d(x, \hat{x})\)</span> and any <span class="arithmatex">\((2^{nR}, n)\)</span> rate distortion code, as long as <span class="arithmatex">\(f_n\)</span> takes on at most <span class="arithmatex">\(2^{nR}\)</span> values and the distortion is less than <span class="arithmatex">\(D\)</span>, the rate is bound to satisfy <span class="arithmatex">\(R \ge R(D)\)</span>.</p>
<p>A pair of sequences <span class="arithmatex">\((x^n, \hat{x}^n)\)</span> is said to be <strong>distortion <span class="arithmatex">\(\epsilon\)</span>-typical</strong> if</p>
<p class="arithmatex">\[
\begin{aligned}
\left|-\frac{1}{n}\log p(x^n) - H(X) \right| &lt; \epsilon, \\
\left|-\frac{1}{n}\log p(\hat{x}^n) - H(\hat{X}) \right| &lt; \epsilon, \\
\left|-\frac{1}{n}\log p(x^n, \hat{x}^n) - H(X, \hat{X}) \right| &lt; \epsilon, \\
\left|d(x^n, \hat{x}^n) - \operatorname{E}d(X, \hat{X})\right| &lt; \epsilon.
\end{aligned}
\]</p>
<p>The set of distortion typical sequences is called <strong>distortion typical set</strong> <span class="arithmatex">\(A_{d, \epsilon}^{(n)}\)</span>.</p>
<p>For <span class="arithmatex">\((X_i, \hat{X}_i)\)</span> drawn i.i.d. <span class="arithmatex">\(\sim p(x, \hat{x})\)</span>, <span class="arithmatex">\(\Pr(A_{d, \epsilon}^{(n)}) \rightarrow 1\)</span> as <span class="arithmatex">\(n \rightarrow \infty\)</span>.</p>
<p>For <span class="arithmatex">\((x^n, \hat{x}^n)\in A_{d, \epsilon}^{(n)}\)</span>, <span class="arithmatex">\(p(\hat{x}^n) \ge p(\hat{x}^n|x^n)2^{-n(I(X; \hat{X})+3\epsilon)}\)</span>.</p>
<div class="admonition abstract">
<p class="admonition-title">Proof</p>
<p class="arithmatex">\[
\begin{aligned}
p(\hat{x}^n|x^n)
&amp;= \frac{p(x^n, \hat{x}^n)}{p(x^n)} \\
&amp;= p(\hat{x}^n)\frac{p(x^n, \hat{x}^n)}{p(x^n)p(\hat{x}^n)} \\
&amp;\le p(\hat{x}^n) \frac{2^{-n(H(X, \hat{X}) - \epsilon)}}{2^{-n(H(X) + \epsilon)}2^{-n(H(\hat{X}) + \epsilon)}} \\
&amp;= p(\hat{x}^n) 2^{n(I(X;\hat{X})+3\epsilon)}.
\end{aligned}
\]</p>
</div>
<h2 id="ch-11-information-theory-and-statistics">Ch. 11: Information theory and statistics<a class="headerlink" href="#ch-11-information-theory-and-statistics" title="Permanent link">&para;</a></h2>
<p>The <strong>type</strong> of a sequence <span class="arithmatex">\(x^n\)</span> is an empirical probability distribution,</p>
<p class="arithmatex">\[
P_{x^n}(x) = \frac{N(x \text{ in } x^n)}{n}.
\]</p>
<p>The <strong>probability simplex</strong> in <span class="arithmatex">\(\mathbb{R}^m\)</span> is an <span class="arithmatex">\((m-1)\)</span>-dim manifold,</p>
<p class="arithmatex">\[
\left\{x^m \in \mathbb{R}^m: x_i \ge 0, \sum x_i = 1\right\}.
\]</p>
<p><span class="arithmatex">\(\mathcal{P}_n\)</span> denotes the set of types with denominator <span class="arithmatex">\(n\)</span>.</p>
<p>The <strong>type class</strong> <span class="arithmatex">\(T(P) = \{x^n \in \mathcal{X}^n: P_{x^n} = P \in \mathcal{P}_n\}\)</span> is the set of sequence of length <span class="arithmatex">\(n\)</span> and type <span class="arithmatex">\(P\)</span>.</p>
<div class="admonition note">
<p class="admonition-title">Example</p>
<p><span class="arithmatex">\(\mathcal{X} = \{1, 2, 3\}\)</span>, <span class="arithmatex">\(x^n = 11321\)</span>.</p>
<p>The type <span class="arithmatex">\(P_{x^n}\)</span>: <span class="arithmatex">\(P_{x^n}(1) = \dfrac{3}{5}\)</span>, <span class="arithmatex">\(P_{x^n}(2) = \dfrac{1}{5}\)</span>, <span class="arithmatex">\(P_{x^n}(3) = \dfrac{1}{5}\)</span>.</p>
<p>The type class <span class="arithmatex">\(T(P_{x^n}) = \{11123, 11132, \cdots \}\)</span>.</p>
<p><span class="arithmatex">\(|T(P_{x^n})| = \dfrac{5!}{3!} = 20\)</span>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Example</p>
<p><span class="arithmatex">\(\mathcal{X} = \{0, 1\}\)</span>.</p>
<p><span class="arithmatex">\(\displaystyle \mathcal{P}_n = \left\{(P(0), P(1)): \left(\frac{0}{n}, \frac{n}{n}\right), \left(\frac{1}{n}, \frac{n-1}{n}\right), \cdots, \left(\frac{n}{n}, \frac{0}{n}\right)\right\}\)</span></p>
</div>
<p class="arithmatex">\[
|\mathcal{P}_n| \le (n+1)^{|\mathcal{X}|},
\]</p>
<p>where the right hand side of the inequality is the scenario where the items in the tuple <span class="arithmatex">\((P(a_1), P(a_2), \cdots, P(a_{|\mathcal{X}|}))\)</span> each takes <span class="arithmatex">\(n+1\)</span> values.</p>
<p>If <span class="arithmatex">\(X_1, X_2, \cdots, X_n\)</span> are drawn i.i.d. from <span class="arithmatex">\(Q(x)\)</span>,</p>
<p class="arithmatex">\[
Q(x^n) = 2^{-n(H(P_{x^n}) + D(P_{x^n} \| Q))}
\]</p>
<div class="admonition abstract">
<p class="admonition-title">Proof</p>
<p class="arithmatex">\[
\begin{aligned}
Q(x^n)
&amp;= \prod_{a\in\mathcal{X}}Q(a)^{N(a\text{ in }x^n)} \\
&amp;= \prod_{a\in\mathcal{X}}Q(a)^{nP_{x^n}(a)} \\
&amp;= \prod_{a\in\mathcal{X}}2^{nP_{x^n}(a)\log Q(a)} \\
&amp;= 2^{n(-H(P_{x^n}) + D(P_{x^n} || Q))}
\end{aligned}
\]</p>
</div>
<p>If <span class="arithmatex">\(x^n \in T(Q)\)</span>, <span class="arithmatex">\(P_{x^n} = Q \Rightarrow Q^n(x^n) = 2^{-nH(Q)}\)</span>.</p>
<p><strong>Size of type class</strong>: for any <span class="arithmatex">\(P \in \mathcal{P}_n\)</span>, <span class="arithmatex">\(T(P) = \dbinom{n}{nP(a_1), \cdots, nP(a_{|\mathcal{X}|})} = \dfrac{n!}{(nP(a_1))!\cdots(nP(a_{|\mathcal{X}|}))!}\)</span>, its bound</p>
<p class="arithmatex">\[
\frac{1}{(n+1)^{|\mathcal{X}|}}2^{nH(P)} \le |T(P)| \le 2^{nH(P)}.
\]</p>
<p>If the alphabet is binary, the type is defined by number of 1's in the sequence, <span class="arithmatex">\(|T(P)| = \dbinom{n}{k, n-k} = \dbinom{n}{k}\)</span>, <span class="arithmatex">\(P_{x^n} = \left(\dfrac{n-k}{n}, \dfrac{k}{n}\right)\)</span>. In this case, a stronger bound for <span class="arithmatex">\(|T(P)|\)</span> is</p>
<p class="arithmatex">\[
\frac{1}{n+1}2^{nH\left(\frac{k}{n}\right)} \le \binom{n}{k} \le 2^{nH\left(\frac{k}{n}\right)},
\]</p>
<p>that is, set <span class="arithmatex">\(|\mathcal{X}|\)</span> to 1.</p>
<p><strong>Stirling's approximation</strong>: for all <span class="arithmatex">\(n \ge 1\)</span>,</p>
<p class="arithmatex">\[
\sqrt{2\pi n}\left(\frac{n}{\mathrm{e}}\right)^n &lt; \sqrt{2\pi n}\left(\frac{n}{\mathrm{e}}\right)^n \mathrm{e}^{\frac{1}{12n + 1}} &lt; n! &lt; \sqrt{2\pi n}\left(\frac{n}{\mathrm{e}}\right)^n \mathrm{e}^{\frac{1}{12n}}.
\]</p>
<p><strong>Probability of type class</strong>: for <span class="arithmatex">\(P \in \mathcal{P}_n\)</span>:</p>
<p class="arithmatex">\[
\frac{1}{(n+1)^{|\mathcal{X}|}}2^{-nD(P||Q)} \le Q^n(T(P)) \le 2^{-nD(P||Q)}.
\]</p>
<p>Given distribution <span class="arithmatex">\(Q\)</span>, the <strong>typical set</strong> is</p>
<p class="arithmatex">\[
T_Q^\epsilon = \{x^n: D(P_{x^n} \| Q) \le \epsilon \}.
\]</p>
<p class="arithmatex">\[
\Pr(D(P_{x^n} \| Q) \ge \epsilon) \le 2^{-n\left(\epsilon - |\mathcal{X}|\frac{\log(n+1)}{n}\right)}.
\]</p>
<p>The <strong>strongly typical set</strong> <span class="arithmatex">\(A^{*(n)}_\epsilon\)</span> is the set of sequences in <span class="arithmatex">\(\mathcal{X}^n\)</span> for which the sample frequencies are close to true values:</p>
<p class="arithmatex">\[
A^{*(n)}_\epsilon =
\left\{ x^n \in \mathcal{X}^n:
\begin{aligned}
&amp;\left| \frac{1}{n}N(a \text{ in } x^n) - P(a) \right| &lt; \frac{\epsilon}{|\mathcal{X}|}, &amp; P(a) &gt; 0 \\
&amp;N(a \text{ in } x^n) = 0, &amp; P(a) = 0
\end{aligned}
\right\}
\]</p>
<p>A <strong>fixed-rate block code</strong> of rate <span class="arithmatex">\(R\)</span> for source <span class="arithmatex">\(X_i\)</span> of unknown distribution <span class="arithmatex">\(Q\)</span>, has</p>
<ul>
<li>encoder <span class="arithmatex">\(f_n: \mathcal{X}^n \mapsto \{1, 2, \cdots, 2^{nR}\}\)</span>,</li>
<li>decoder <span class="arithmatex">\(\phi_n: \{1, 2, \cdots, 2^{nR}\} \mapsto \mathcal{X}^n\)</span>.</li>
</ul>
<p>Its probability of error</p>
<p class="arithmatex">\[
P_e^{(n)} = Q^n(X^n: \phi_n f_n(X^n) \neq X^n)
\]</p>
<p>A rate <span class="arithmatex">\(R\)</span> block code is called <strong>universal</strong> if <span class="arithmatex">\(f_n\)</span> and <span class="arithmatex">\(\phi_n\)</span> do not depend on <span class="arithmatex">\(Q\)</span>, and if <span class="arithmatex">\(P_e^{(n)} \rightarrow 0\)</span> as <span class="arithmatex">\(n \rightarrow 0\)</span> if <span class="arithmatex">\(R &gt; H(Q)\)</span>.</p>
<p><strong>Universal source coding</strong>: for any distribution <span class="arithmatex">\(Q\)</span> with <span class="arithmatex">\(H(Q) &lt; R\)</span>, let <span class="arithmatex">\(R_n = R - |\mathcal{X}|\frac{\log(n+1)}{n}\)</span>, <span class="arithmatex">\(A = \{x^n\in \mathcal{X}^n: H(P_{x^n}) \le R_n\}\)</span>, then <span class="arithmatex">\(|A| \le 2^{nR}\)</span>. Encode and decode codewords/messages in <span class="arithmatex">\(A\)</span> only, then</p>
<p class="arithmatex">\[
P_e^{(n)} = 1 - Q^n(A) \le (n+1)^{|\mathcal{X}|}2^{-n\min\limits_{P: H(P) &gt; R_n}D(P\|Q)}.
\]</p>
<p>Since <span class="arithmatex">\(R_n \rightarrow R\)</span> as <span class="arithmatex">\(n \rightarrow \infty\)</span>, and <span class="arithmatex">\(H(Q) &lt; R\)</span> thus <span class="arithmatex">\(R_n &gt; H(Q)\)</span> for <span class="arithmatex">\(n\)</span> sufficiently large.</p>
<p><strong>Sanov's theorem</strong>: <span class="arithmatex">\(X_1, X_2, \cdots, X_n\)</span> i.i.d. drawn <span class="arithmatex">\(\sim Q(x)\)</span>, <span class="arithmatex">\(E\subseteq \mathcal{P}\)</span> is a set of probability distributions,</p>
<p class="arithmatex">\[
Q^n(E) = Q^n(E \cap P_n) \le (n+1)^{|\mathcal{X}|}2^{-nD(P^*\|Q)},
\]</p>
<p>where <span class="arithmatex">\(P^* = \argmin\limits_{P\in E} D(P \| Q)\)</span>. If <span class="arithmatex">\(E\)</span> is the closure of its interior,</p>
<p class="arithmatex">\[
Q^n(E) \rightarrow 2^{-nD(P^*\|Q)}.
\]</p>
<div class="admonition note">
<p class="admonition-title">Example: Dice</p>
<p>Toss a die <span class="arithmatex">\(n\)</span> times, derive the probability of the average points <span class="arithmatex">\(\ge 4\)</span>.</p>
<p class="arithmatex">\[
\sum_{i=1}^6 iP(i) \ge 4,
\]</p>
<p class="arithmatex">\[
P^*(x) = \frac{2^{\lambda x}}{\sum\limits_{i=1}^6 2^{\lambda i}}.
\]</p>
</div>
<div class="admonition note">
<p class="admonition-title">Example: Coin</p>
<p>Toss a coin <span class="arithmatex">\(n\)</span> times, derive the probability of resulting in more than 7 heads.</p>
<p class="arithmatex">\[
P(\bar{X}_n \ge 0.7) \approx 2^{-nD(P^*\|Q)},
\]</p>
<p><span class="arithmatex">\(P^* = (0.7, 0.3)\)</span>, <span class="arithmatex">\(Q = (0.5, 0.5)\)</span>.</p>
</div>
<p>For a closed convex set <span class="arithmatex">\(E\subset \mathcal{P}\)</span> and distribution <span class="arithmatex">\(Q \not\in E\)</span>, if <span class="arithmatex">\(D(P^*\| Q) = \min\limits_{P \in E} D(P \| Q)\)</span>,</p>
<p class="arithmatex">\[
D(P \| Q) \ge D(P \| P^*) + D(P^* \| Q), \quad P \in E.
\]</p>
<p>Suppose <span class="arithmatex">\(P_n \in E\)</span> yields <span class="arithmatex">\(D(P_n \| Q) \rightarrow D(P^* \| Q)\)</span> then <span class="arithmatex">\(D(P_n \| P) \rightarrow 0\)</span>.</p>
<p><span class="arithmatex">\(L_i\)</span> <strong>distance</strong> between two distributions</p>
<p class="arithmatex">\[
\|P_1 - P_2\|_1 = \sum_{a\in\mathcal{X}} |P_1(a) - P_2(a)|.
\]</p>
<p><span class="arithmatex">\(D(P_1 \| P_2) \ge \dfrac{1}{2\ln 2}\|P_1 - P_2\|_1^2\)</span>.</p>
<p><strong>Conditional limit theorem</strong>: by definitions above,</p>
<p class="arithmatex">\[
\Pr(X_1 = a \mid P_{x^n} \in E)\rightarrow P^*(a) \quad \text{in probability, as } n\rightarrow \infty.
\]</p>
<div class="admonition note">
<p class="admonition-title">Sanov's th. and conditional limit th.</p>
<p>Sanov's th.: probability of <span class="arithmatex">\(E\)</span> is essentially the same as that of the closest type <span class="arithmatex">\(P^*\)</span> in <span class="arithmatex">\(E\)</span> to <span class="arithmatex">\(Q\)</span>.</p>
<p>Conditional limit th.: and other types far away from <span class="arithmatex">\(P^*\)</span> is negligible.</p>
</div>
<p><strong>Hypothesis testing</strong>: <span class="arithmatex">\(X_1, X_2, \cdots, X_n\)</span> i.i.d. drawn <span class="arithmatex">\(\sim Q(x)\)</span>.</p>
<ul>
<li>Hypotheses:<ul>
<li><span class="arithmatex">\(H_1: Q = P_1\)</span>,</li>
<li><span class="arithmatex">\(H_2: Q = P_2\)</span>.</li>
</ul>
</li>
<li>The decision function <span class="arithmatex">\(g(X_1, X_2, \cdots, X_n) = i \in \{1, 2\}\)</span> indicates which hypothesis is accepted.</li>
</ul>
<p><span class="arithmatex">\(A = \{x^n\in \mathcal{X}^n: g(x^n) = 1\}\)</span>, the two probabilities of error:</p>
<p class="arithmatex">\[
\alpha = \Pr(g(X^n) = 2 \mid H_1) = P_1^n(A^C),
\]</p>
<p class="arithmatex">\[
\beta = \Pr(g(X^n) = 1 \mid H_2) = P_2^n(A).
\]</p>
<p><strong>Neyman-Pearson lemma</strong>: for <span class="arithmatex">\(T\ge 0\)</span>, decision region</p>
<p class="arithmatex">\[
A_n(T) = \left\{x^n: \frac{P_1(x^n)}{P_2(x^n)} &gt; T \right\}
\]</p>
<p>has corresponding probability of error <span class="arithmatex">\(\alpha^*\)</span> and <span class="arithmatex">\(\beta^*\)</span>, and <span class="arithmatex">\(B_n\)</span> be any other decision region with associated <span class="arithmatex">\(\alpha\)</span> and <span class="arithmatex">\(\beta\)</span>. If <span class="arithmatex">\(\alpha \le \alpha^*\)</span>, <span class="arithmatex">\(\beta \ge \beta^*\)</span>.</p>
<p>Therefore, the optimum test for two hypotheses is of the form</p>
<p class="arithmatex">\[
\frac{P_1(x^n)}{P_2(x^n)} &gt; T,
\]</p>
<p>where the left hand side is called the likelihood ratio.</p>
<p><strong>AEP for relative entropy</strong>: <span class="arithmatex">\(X^n\)</span> i.i.d. drawn from <span class="arithmatex">\(P_1\)</span> and <span class="arithmatex">\(P_2\)</span> is any other distribution,</p>
<p class="arithmatex">\[
\frac{1}{n}\log \frac{P_1(x^n)}{P_2(x^n)} \rightarrow D(P_1 \| P_2) \quad \text{in probability}.
\]</p>
<p>A <strong>relative entropy typical set</strong> <span class="arithmatex">\(A_\epsilon^{(n)}(P_1 \| P_2)\)</span> consists of <span class="arithmatex">\(x^n \in \mathcal{X}^n\)</span> such that</p>
<p class="arithmatex">\[
D(P_1 \| P_2) - \epsilon \le \frac{1}{n}\log \frac{P_1(x^n)}{P_2(x^n)} \le D(P_1 \| P_2) + \epsilon.
\]</p>
<ul>
<li><span class="arithmatex">\(P_1(x^n)2^{-n(D(P_1\|P_2) + \epsilon)} \le P_2(x^n) \le P_1(x^n)2^{-n(D(P_1\|P_2) - \epsilon)}\)</span>, for all <span class="arithmatex">\(x^n \in A_\epsilon^{(n)}(P_1 \| P_2)\)</span>.</li>
<li><span class="arithmatex">\(P_1(A_\epsilon^{(n)}(P_1 \| P_2)) &gt; 1 - \epsilon\)</span>, for <span class="arithmatex">\(n\)</span> sufficiently large.</li>
<li><span class="arithmatex">\(P_2(A_\epsilon^{(n)}(P_1 \| P_2)) &lt; 2^{-n(D(P_1\|P_2) - \epsilon)}\)</span>.</li>
<li><span class="arithmatex">\(P_2(A_\epsilon^{(n)}(P_1 \| P_2)) &gt; (1 - \epsilon)2^{-n(D(P_1\|P_2) + \epsilon)}\)</span>, for <span class="arithmatex">\(n\)</span> sufficiently large.</li>
</ul>
<p><strong>Chernoff-Stein lemma</strong>: let <span class="arithmatex">\(X^n\)</span> be i.i.d. <span class="arithmatex">\(\sim Q\)</span>, hypotheses <span class="arithmatex">\(Q = P_1\)</span> and <span class="arithmatex">\(Q = P_2\)</span>, where <span class="arithmatex">\(D(P_1 \| P_2) &lt; \infty\)</span>. Let <span class="arithmatex">\(A_n \subseteq \mathcal{X}^n\)</span> be an acceptance region for <span class="arithmatex">\(H_1\)</span>. For <span class="arithmatex">\(0 &lt; \epsilon &lt; 1/2\)</span>, define <span class="arithmatex">\(\beta_n^\epsilon = \min\limits_{A_n \subseteq \mathcal{X}^n,\ \alpha_n \le \epsilon} \beta_n\)</span>, then</p>
<p class="arithmatex">\[
\lim_{n\rightarrow \infty} \frac{1}{n}\log \beta_n^\epsilon = -D(P_1 \| P_2).
\]</p>
<p>An estimator for <span class="arithmatex">\(\theta\)</span> for sample size <span class="arithmatex">\(n\)</span> is a function <span class="arithmatex">\(T: \mathcal{X}^n \mapsto \varTheta\)</span>. Denote true distribution of <span class="arithmatex">\(X\)</span> by <span class="arithmatex">\(f(x; \theta)\)</span>.</p>
<ul>
<li>The bias is <span class="arithmatex">\(\operatornamewithlimits{E}\limits_{f(\cdot; \theta)}(T(X^n) - \theta)\)</span>. The estimator is <strong>unbiased</strong> if bias is zero for all <span class="arithmatex">\(\theta\)</span>.</li>
<li>The estimator is <strong>consistent</strong> in probability if <span class="arithmatex">\(T(X^n) \rightarrow \theta\)</span> as <span class="arithmatex">\(n \rightarrow \infty\)</span>.</li>
<li><span class="arithmatex">\(T_1\)</span> <strong>dominates</strong> <span class="arithmatex">\(T_2\)</span> if <span class="arithmatex">\(\operatorname{E}(T_1(X^n) - \theta)^2 \le \operatorname{E}(T_2(X^n) - \theta)^2\)</span>.</li>
</ul>
<p>The <strong>score/informant</strong></p>
<p class="arithmatex">\[
V = \frac{\partial}{\partial \theta}\ln f(X; \theta) = \frac{\dfrac{\partial}{\partial \theta} f(X; \theta)}{f(X; \theta)}.
\]</p>
<p><span class="arithmatex">\(\operatorname{E}V = 0\)</span>, and <span class="arithmatex">\(\operatorname{E}V^2 = \operatorname{Var}(V)\)</span>.</p>
<p>The <strong>Fisher information</strong></p>
<p class="arithmatex">\[
J(\theta) = \operatorname{E}_\theta V^2 = \operatorname{E}_\theta\left[\frac{\partial}{\partial \theta}\ln f(X; \theta)\right]^2.
\]</p>
<p>If <span class="arithmatex">\(X_1, X_2, \cdots, X_n\)</span> are i.i.d. drawn from <span class="arithmatex">\(f(x; \theta)\)</span>,</p>
<p class="arithmatex">\[
V(X^n) = \sum_{i=1}^n V(X_i), \quad J_n(\theta) = nJ(\theta).
\]</p>
<p><strong>Cramer-Rao inequality</strong>: the mean-squared error of any unbiased estimator <span class="arithmatex">\(T\)</span> of parameter <span class="arithmatex">\(\theta\)</span> satisfies</p>
<p class="arithmatex">\[
\operatorname{Var}(T) \ge \frac{1}{J(\theta)}.
\]</p>
<p>An unbiased estimator <span class="arithmatex">\(T\)</span> is said to be <strong>efficient</strong> if <span class="arithmatex">\(\operatorname{Var}(T) = \dfrac{1}{J(\theta)}\)</span>.</p>
<div class="admonition note">
<p class="admonition-title">Example</p>
<p>Let <span class="arithmatex">\(X_1, X_2, \cdots, X_n\)</span> be i.i.d. <span class="arithmatex">\(\sim N(\theta, \sigma^2)\)</span>, <span class="arithmatex">\(\sigma^2\)</span> known.</p>
<p>The score</p>
<p class="arithmatex">\[
\begin{aligned}
V
&amp;= \frac{\partial}{\partial \theta}\ln N(x; \theta, \sigma^2) \\
&amp;= \frac{\partial}{\partial \theta}\ln \left( \frac{1}{\sigma\sqrt{2\pi\mathrm{e}}}\mathrm{e}^{-\frac{1}{2}\left(\frac{x - \theta}{\sigma}\right)^2}\right) \\
&amp;= \frac{x - \theta}{\sigma^2}.
\end{aligned}
\]</p>
<p>Fisher information</p>
<p class="arithmatex">\[
\begin{aligned}
J(\theta)
&amp;= \operatorname{E}V^2 \\
&amp;= \frac{1}{\sigma^4}\left(\operatorname{E}X^2 - 2\operatorname{E}(\theta X) + \operatorname{E}\theta^2 \right) \\
&amp;= \frac{1}{\sigma^2}.
\end{aligned}
\]</p>
<p class="arithmatex">\[
J_n(\theta) = nJ(\theta) = \dfrac{n}{\theta^2}.
\]</p>
<p>One estimator for <span class="arithmatex">\(\theta\)</span> is <span class="arithmatex">\(T = \sum\limits_{i=1}^n X_i\)</span>.</p>
<p class="arithmatex">\[
\begin{aligned}
\operatorname{Var}(T)
&amp;= \operatorname{E}\left(\bar{X}_n\right)^2 - \left(\operatorname{E}\bar{X}_n\right)^2 \\
&amp;= \frac{1}{n^2}\operatorname{E}\textstyle\sum X_i - \theta^2 \\
&amp;= \frac{1}{n^2}\left(\operatorname{Var}(\textstyle\sum X_i) + \left(\operatorname{E}\textstyle\sum X\right)^2\right) - \theta^2 \\
&amp;= \frac{n\sigma^2 + (n\theta)^2}{n^2} - \theta^2 \\
&amp;= \frac{\sigma^2}{n}.
\end{aligned}
\]</p>
<p>Therefore, <span class="arithmatex">\(\operatorname{Var}(T) = \dfrac{1}{J_n(\theta)}\)</span>, <span class="arithmatex">\(T\)</span> is the minimum variance unbiased (efficient) estimator of <span class="arithmatex">\(\theta\)</span>.</p>
<div class="admonition info">
<p>Note that <span class="arithmatex">\(\operatorname{E}X^2 = \operatorname{Var}(X) + (\operatorname{E}X)^2\)</span>, and for <span class="arithmatex">\(X \sim N(\mu, \sigma^2)\)</span>, <span class="arithmatex">\(\operatorname{E}X^2 = \mu^2 + \sigma^2\)</span>.</p>
</div>
</div>


  




                
              </article>
            </div>
          
          
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2017 - 2023 <a href="https://github.com/x4Cx58x54" target="_blank">x4Cx58x54</a> </br> Licensed under <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a>
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/x4Cx58x54" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.3.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.tabs", "navigation.indexes", "navigation.instant", "content.code.copy"], "search": "../../assets/javascripts/workers/search.208ed371.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.19047be9.min.js"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.js"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/auto-render.min.js"></script>
      
        <script src="../../_static/js/extra.js"></script>
      
    
  </body>
</html>